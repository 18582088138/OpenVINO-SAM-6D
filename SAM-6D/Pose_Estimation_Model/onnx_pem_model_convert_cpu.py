import gorilla
import argparse
import os
import sys
from PIL import Image
import os.path as osp
import numpy as np
import random
import importlib
import json

import torch
import torchvision.transforms as transforms
import cv2

import torch.nn as nn
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.join(BASE_DIR, '..', 'Pose_Estimation_Model')
sys.path.append(os.path.join(ROOT_DIR, 'provider'))
sys.path.append(os.path.join(ROOT_DIR, 'utils'))
sys.path.append(os.path.join(ROOT_DIR, 'model'))
sys.path.append(os.path.join(BASE_DIR, 'model', 'pointnet2'))


def get_parser():
    parser = argparse.ArgumentParser(
        description="Pose Estimation Model Convert (CPU Version)")
    # pem
    parser.add_argument("--device",
                        type=str,
                        default="cpu",
                        help="device to run on (cpu/cuda)")
    parser.add_argument("--model",
                        type=str,
                        default="pose_estimation_model",
                        help="path to model file")
    parser.add_argument("--config",
                        type=str,
                        default="config/base.yaml",
                        help="path to config file, different config.yaml use different config")
    parser.add_argument("--iter",
                        type=int,
                        default=600000,
                        help="epoch num. for testing")
    parser.add_argument("--exp_id",
                        type=int,
                        default=0,
                        help="")
    
    # Set default input parameter, reference demo.sh
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
    default_cad_path = os.path.join(project_root, 'SAM-6D/Data/Example/obj_000005.ply')
    default_rgb_path = os.path.join(project_root, 'SAM-6D/Data/Example/rgb.png')
    default_depth_path = os.path.join(project_root, 'SAM-6D/Data/Example/depth.png')
    default_camera_path = os.path.join(project_root, 'SAM-6D/Data/Example/camera.json')
    default_output_dir = os.path.join(project_root, 'SAM-6D/Data/Example/outputs')
    default_seg_path = os.path.join(default_output_dir, 'sam6d_results/detection_ism.json')

    parser.add_argument("--output_dir", nargs="?", default=default_output_dir, help="Path to root directory of the output")
    parser.add_argument("--cad_path", nargs="?", default=default_cad_path, help="Path to CAD(mm)")
    parser.add_argument("--rgb_path", nargs="?", default=default_rgb_path, help="Path to RGB image")
    parser.add_argument("--depth_path", nargs="?", default=default_depth_path, help="Path to Depth image(mm)")
    parser.add_argument("--cam_path", nargs="?", default=default_camera_path, help="Path to camera information")
    parser.add_argument("--seg_path", nargs="?", default=default_seg_path, help="Path to segmentation information(generated by ISM)")
    parser.add_argument("--det_score_thresh", default=0.2, help="The score threshold of detection")
    args_cfg = parser.parse_args()

    return args_cfg

def init():
    args = get_parser()
    exp_name = args.model + '_' + \
        osp.splitext(args.config.split("/")[-1])[0] + '_id' + str(args.exp_id)
    log_dir = osp.join("log", exp_name)

    cfg = gorilla.Config.fromfile(args.config)
    cfg.exp_name = exp_name
    cfg.device = args.device
    cfg.model_name = args.model
    cfg.log_dir = log_dir
    cfg.test_iter = args.iter

    cfg.output_dir = args.output_dir
    cfg.cad_path = args.cad_path
    cfg.rgb_path = args.rgb_path
    cfg.depth_path = args.depth_path
    cfg.cam_path = args.cam_path
    cfg.seg_path = args.seg_path

    cfg.det_score_thresh = args.det_score_thresh
    
    # 检查设备可用性
    if cfg.device == "cuda" and not torch.cuda.is_available():
        print("CUDA不可用，自动切换到CPU")
        cfg.device = "cpu"
    
    print(f"使用设备: {cfg.device}")

    return cfg


# 尝试导入依赖模块，如果不存在则提供替代实现
try:
    from data_utils import (
        load_im,
        get_bbox,
        get_point_cloud_from_depth,
        get_resize_rgb_choose,
    )
    from draw_utils import draw_detections
except ImportError:
    print("警告: 无法导入data_utils或draw_utils，使用简化实现")
    
    def load_im(path):
        """加载图像文件"""
        if path.endswith('.png') or path.endswith('.jpg') or path.endswith('.jpeg'):
            return cv2.imread(path, cv2.IMREAD_UNCHANGED)
        else:
            return np.load(path)
    
    def get_bbox(mask):
        """获取mask的边界框"""
        rows = np.any(mask, axis=1)
        cols = np.any(mask, axis=0)
        y1, y2 = np.where(rows)[0][[0, -1]]
        x1, x2 = np.where(cols)[0][[0, -1]]
        return y1, y2, x1, x2
    
    def get_point_cloud_from_depth(depth, K):
        """从深度图生成点云"""
        h, w = depth.shape
        fx, fy = K[0, 0], K[1, 1]
        cx, cy = K[0, 2], K[1, 2]
        
        y, x = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')
        x = (x - cx) * depth / fx
        y = (y - cy) * depth / fy
        z = depth
        
        return np.stack([x, y, z], axis=-1)
    
    def get_resize_rgb_choose(choose, bbox, img_size):
        """获取调整大小后的RGB选择索引"""
        y1, y2, x1, x2 = bbox
        h, w = y2 - y1, x2 - x1
        scale_h, scale_w = img_size / h, img_size / w
        
        choose_y = (choose // w).astype(np.float32) * scale_h
        choose_x = (choose % w).astype(np.float32) * scale_w
        choose = choose_y * img_size + choose_x
        return choose.astype(np.int32)
    
    def draw_detections(rgb, pred_rot, pred_trans, model_points, K, color=(255, 0, 0)):
        """绘制检测结果（简化版本）"""
        # 简化实现，只返回原图
        return rgb.copy()

import pycocotools.mask as cocomask
import trimesh

rgb_transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                    std=[0.229, 0.224, 0.225])])

def visualize(rgb, pred_rot, pred_trans, model_points, K, save_path):
    """
    Visualize the predicted pose by drawing the 3D model overlay on the RGB image.
    Args:
        rgb: np.ndarray, shape (H, W, 3), uint8, input RGB image
        pred_rot: np.ndarray, shape (N, 3, 3), predicted rotation matrices
        pred_trans: np.ndarray, shape (N, 3), predicted translations (mm)
        model_points: np.ndarray, shape (M, 3), 3D model points (mm)
        K: np.ndarray, shape (N, 3, 3), camera intrinsics
        save_path: str, path to save the visualization image
    Returns:
        concat: PIL.Image, side-by-side visualization image
    """
    img = draw_detections(rgb, pred_rot, pred_trans, model_points, K, color=(255, 0, 0))
    img = Image.fromarray(np.uint8(img))
    img.save(save_path)
    prediction = Image.open(save_path)
    
    # concat side by side in PIL
    rgb = Image.fromarray(np.uint8(rgb))
    img = np.array(img)
    concat = Image.new('RGB', (img.shape[1] + prediction.size[0], img.shape[0]))
    concat.paste(rgb, (0, 0))
    concat.paste(prediction, (img.shape[1], 0))
    return concat


def _get_template(path, cfg, device, tem_index=1):
    """
    Load a single template (rendered view) for the CAD model.
    Args:
        path: str, directory containing template files
        cfg: config object, must have img_size, n_sample_template_point, rgb_mask_flag
        device: str, device to place tensors on
        tem_index: int, template index
    Returns:
        rgb: torch.Tensor, shape (3, img_size, img_size), normalized RGB image
        rgb_choose: torch.Tensor, shape (n_sample_template_point,), selected pixel indices
        xyz: np.ndarray, shape (n_sample_template_point, 3), 3D points (meters)
    """
    rgb_path = os.path.join(path, 'rgb_'+str(tem_index)+'.png')
    mask_path = os.path.join(path, 'mask_'+str(tem_index)+'.png')
    xyz_path = os.path.join(path, 'xyz_'+str(tem_index)+'.npy')

    rgb = load_im(rgb_path).astype(np.uint8)
    xyz = np.load(xyz_path).astype(np.float32) / 1000.0  
    mask = load_im(mask_path).astype(np.uint8) == 255

    bbox = get_bbox(mask)
    y1, y2, x1, x2 = bbox
    mask = mask[y1:y2, x1:x2]

    rgb = rgb[:,:,::-1][y1:y2, x1:x2, :]
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)

    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    rgb = rgb_transform(np.array(rgb))

    choose = (mask>0).astype(np.float32).flatten().nonzero()[0]
    if len(choose) <= cfg.n_sample_template_point:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point)
    else:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=False)
    choose = choose[choose_idx]
    xyz = xyz[y1:y2, x1:x2, :].reshape((-1, 3))[choose, :]

    rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)
    return rgb, rgb_choose, xyz


def get_templates(path, cfg, device):
    """
    Load multiple rendered templates for the CAD model from disk.
    Args:
        path: str, directory containing template files
        cfg: config object, must have n_template_view, img_size, n_sample_template_point
        device: str, device to place tensors on
    Returns:
        all_tem: list[torch.Tensor], each (1, 3, img_size, img_size)
        all_tem_pts: list[torch.Tensor], each (1, n_sample_template_point, 3)
        all_tem_choose: list[torch.Tensor], each (1, n_sample_template_point)
    """
    n_template_view = cfg.n_template_view
    all_tem = []
    all_tem_choose = []
    all_tem_pts = []

    total_nView = 42
    for v in range(n_template_view):
        i = int(total_nView / n_template_view * v)
        tem, tem_choose, tem_pts = _get_template(path, cfg, device, i)
        all_tem.append(torch.FloatTensor(tem).unsqueeze(0).to(device))
        all_tem_choose.append(torch.IntTensor(tem_choose).long().unsqueeze(0).to(device))
        all_tem_pts.append(torch.FloatTensor(tem_pts).unsqueeze(0).to(device))
    return all_tem, all_tem_pts, all_tem_choose


def get_test_data(rgb_path, depth_path, cam_path, cad_path, seg_path, det_score_thresh, cfg, device):
    """
    Prepare test data for pose estimation.
    Args:
        rgb_path: str, path to RGB image
        depth_path: str, path to depth image
        cam_path: str, path to camera intrinsics (json)
        cad_path: str, path to CAD model
        seg_path: str, path to segmentation results (json)
        det_score_thresh: float, detection score threshold
        cfg: config object, must have n_sample_observed_point, img_size, rgb_mask_flag
        device: str, device to place tensors on
    Returns:
        ret_dict: dict with keys:
            'pts': torch.Tensor, (N, n_sample_observed_point, 3)
            'rgb': torch.Tensor, (N, 3, img_size, img_size)
            'rgb_choose': torch.Tensor, (N, n_sample_observed_point)
            'score': torch.Tensor, (N,)
            'model': torch.Tensor, (N, n_sample_model_point, 3)
            'K': torch.Tensor, (N, 3, 3)
        whole_image: np.ndarray, (H, W, 3), original RGB image
        whole_pts: np.ndarray, (H*W, 3), full point cloud
        model_points: np.ndarray, (n_sample_model_point, 3)
        all_dets: list[dict], detection info
    """
    dets = []
    with open(seg_path) as f:
        dets_ = json.load(f) # keys: scene_id, image_id, category_id, bbox, score, segmentation
    for det in dets_:
        if det['score'] > det_score_thresh:
            dets.append(det)
    del dets_

    cam_info = json.load(open(cam_path))
    K = np.array(cam_info['cam_K']).reshape(3, 3)

    whole_image = load_im(rgb_path).astype(np.uint8)
    if len(whole_image.shape)==2:
        whole_image = np.concatenate([whole_image[:,:,None], whole_image[:,:,None], whole_image[:,:,None]], axis=2)
    whole_depth = load_im(depth_path).astype(np.float32) * cam_info['depth_scale'] / 1000.0
    whole_pts = get_point_cloud_from_depth(whole_depth, K)

    mesh = trimesh.load_mesh(cad_path)
    model_points = mesh.sample(cfg.n_sample_model_point).astype(np.float32) / 1000.0
    radius = np.max(np.linalg.norm(model_points, axis=1))

    all_rgb = []
    all_cloud = []
    all_rgb_choose = []
    all_score = []
    all_dets = []
    for inst in dets:
        seg = inst['segmentation']
        score = inst['score']

        # mask
        h,w = seg['size']
        try:
            rle = cocomask.frPyObjects(seg, h, w)
        except:
            rle = seg
        mask = cocomask.decode(rle)
        mask = np.logical_and(mask > 0, whole_depth > 0)
        if np.sum(mask) > 32:
            bbox = get_bbox(mask)
            y1, y2, x1, x2 = bbox
        else:
            continue
        mask = mask[y1:y2, x1:x2]
        choose = mask.astype(np.float32).flatten().nonzero()[0]

        # pts
        cloud = whole_pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)[choose, :]
        center = np.mean(cloud, axis=0)
        tmp_cloud = cloud - center[None, :]
        flag = np.linalg.norm(tmp_cloud, axis=1) < radius * 1.2
        if np.sum(flag) < 4:
            continue
        choose = choose[flag]
        cloud = cloud[flag]

        if len(choose) <= cfg.n_sample_observed_point:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point)
        else:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
        choose = choose[choose_idx]
        cloud = cloud[choose_idx]

        # rgb
        rgb = whole_image.copy()[y1:y2, x1:x2, :][:,:,::-1]
        if cfg.rgb_mask_flag:
            rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)
        rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
        rgb = rgb_transform(np.array(rgb))
        rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)

        all_rgb.append(torch.FloatTensor(rgb))
        all_cloud.append(torch.FloatTensor(cloud))
        all_rgb_choose.append(torch.IntTensor(rgb_choose).long())
        all_score.append(score)
        all_dets.append(inst)

    ret_dict = {}
    ret_dict['pts'] = torch.stack(all_cloud).to(device)
    ret_dict['rgb'] = torch.stack(all_rgb).to(device)
    ret_dict['rgb_choose'] = torch.stack(all_rgb_choose).to(device)
    ret_dict['score'] = torch.FloatTensor(all_score).to(device)

    ninstance = ret_dict['pts'].size(0)
    ret_dict['model'] = torch.FloatTensor(model_points).unsqueeze(0).repeat(ninstance, 1, 1).to(device)
    ret_dict['K'] = torch.FloatTensor(K).unsqueeze(0).repeat(ninstance, 1, 1).to(device)
    return ret_dict, whole_image, whole_pts.reshape(-1, 3), model_points, all_dets


class PEMWrapperModel(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.input_keys = [
            'pts', 'rgb', 'rgb_choose', 'score', 'model', 'K',
            'dense_po', 'dense_fo'
        ]
        self.output_key = 'pred_t'

    def forward(self, pts, rgb, rgb_choose, score, model_pts, K,
                dense_po, dense_fo):
        # 适配重构后的Net，直接参数传递
        return self.model(pts, rgb, rgb_choose, score, model_pts, K, dense_po, dense_fo)


if __name__ == "__main__":
    cfg = init()

    random.seed(cfg.rd_seed)
    torch.manual_seed(cfg.rd_seed)

    # 设置设备
    device = torch.device(cfg.device)
    print(f"使用设备: {device}")

    # model
    print("=> creating model ...")
    MODEL = importlib.import_module(cfg.model_name)
    model = MODEL.Net(cfg.model)
    model = model.to(device)
    model.eval()
    checkpoint = os.path.join(os.path.dirname((os.path.abspath(__file__))), 'checkpoints', 'sam-6d-pem-base.pth')
    
    # 加载checkpoint时指定map_location
    gorilla.solver.load_checkpoint(model=model, filename=checkpoint, map_location=device)

    print("=> extracting templates ...")
    tem_path = os.path.join(cfg.output_dir, 'templates')
    all_tem, all_tem_pts, all_tem_choose = get_templates(tem_path, cfg.test_dataset, device)
    with torch.no_grad():
        all_tem_pts, all_tem_feat = model.feature_extraction.get_obj_feats(all_tem, all_tem_pts, all_tem_choose)

    print("=> loading input data ...")
    input_data, img, whole_pts, model_points, detections = get_test_data(
        cfg.rgb_path, cfg.depth_path, cfg.cam_path, cfg.cad_path, cfg.seg_path, 
        cfg.det_score_thresh, cfg.test_dataset, device
    )
    print(f"[ONNX] input_data.keys: {input_data.keys()}")
    ninstance = input_data['pts'].size(0)
    
    print("=> running model ...")
    with torch.no_grad():
        input_data['dense_po'] = all_tem_pts.repeat(ninstance,1,1)
        input_data['dense_fo'] = all_tem_feat.repeat(ninstance,1,1)
        model_input_tuple = (
            input_data['pts'], input_data['rgb'], input_data['rgb_choose'], input_data['score'],
            input_data['model'], input_data['K'], input_data['dense_po'], input_data['dense_fo']
        )
        pred_R, pred_t, pred_pose_score = model(*model_input_tuple)

    # 直接用Net导出，不再用PEMWrapperModel
    example_inputs = model_input_tuple
    onnx_input_name = ["pts", "rgb", "rgb_choose", "score", "model", "K", "dense_po", "dense_fo"]
    onnx_output_name = ["pred_R", "pred_t", "pred_pose_score"]
    onnx_model_path = "pose_estimation_model_cpu.onnx"

    print("=> 尝试ONNX导出...")
    try:
        torch.onnx.export(
            model,
            example_inputs,
            onnx_model_path,
            opset_version=20,
            operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK,
            input_names=onnx_input_name,
            output_names=onnx_output_name,
            dynamic_axes={k: {0: "batch"} for k in onnx_input_name},
            do_constant_folding=False,
        )
        print(f"[ONNX] PEM ONNX model export success: {onnx_model_path}")
            
    except Exception as e:
        print(f"[ONNX] export failed : {e}")

    # 保存PyTorch推理结果
    # pose_scores = pred_pose_score.detach().cpu().numpy()
    pose_scores = pred_pose_score.detach().cpu().numpy() * input_data['score'].detach().cpu().numpy()
    pred_rot = pred_R.detach().cpu().numpy()
    pred_trans = pred_t.detach().cpu().numpy() * 1000

    print("=> saving results ...")
    os.makedirs(f"{cfg.output_dir}/sam6d_results", exist_ok=True)
    for idx, det in enumerate(detections):
        det['score'] = float(pose_scores[idx])
        det['R'] = list(pred_rot[idx].tolist())
        det['t'] = list(pred_trans[idx].tolist())

    with open(os.path.join(f"{cfg.output_dir}/sam6d_results", f'detection_pem_{cfg.device}.json'), "w") as f:
        json.dump(detections, f)

    print("=> visualizating ...")
    save_path = os.path.join(f"{cfg.output_dir}/sam6d_results", f'vis_pem_{cfg.device}.png')
    valid_masks = pose_scores == pose_scores.max()
    K = input_data['K'].detach().cpu().numpy()[valid_masks]
    vis_img = visualize(img, pred_rot[valid_masks], pred_trans[valid_masks], model_points*1000, K, save_path)
    vis_img.save(save_path)
    print(f"[Inference Done] Pose_Estimation_Model ({cfg.device} Version)") 