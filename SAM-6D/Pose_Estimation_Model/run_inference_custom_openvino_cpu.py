# Suppress NVML errors when CUDA is not available
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# Environment setup for CUDA compatibility
def setup_cuda_environment():
    """Setup CUDA environment and handle NVML errors gracefully"""
    # Check CUDA availability
    cuda_available = torch.cuda.is_available()
    
    # Try to import gpustat but handle the case when NVML is not available
    gpustat_module = None
    try:
        import gpustat
        gpustat_module = gpustat
        if cuda_available:
            print("GPU monitoring enabled")
        else:
            print("CUDA not available, GPU monitoring disabled")
    except ImportError:
        print("Warning: gpustat not available, GPU monitoring disabled")
    except Exception as e:
        if "NVML" in str(e) or "nvidia" in str(e).lower():
            print("Warning: NVIDIA Management Library not available, GPU monitoring disabled")
        else:
            print(f"Warning: Error importing gpustat: {e}")
    
    return cuda_available, gpustat_module

# Try to import gorilla with error handling
gorilla_module = None
try:
    import gorilla
    gorilla_module = gorilla
    print("Gorilla library imported successfully")
except Exception as e:
    if "NVML" in str(e) or "nvidia" in str(e).lower():
        print("Warning: Gorilla library not available due to NVML error, using fallback configuration")
        gorilla_module = None
    else:
        print(f"Warning: Error importing gorilla: {e}")
        gorilla_module = None

import argparse
import os
import sys
from PIL import Image
import os.path as osp
import numpy as np
import time
import json
import cv2

import torch
from openvino import Core

# Setup environment
CUDA_AVAILABLE, gpustat = setup_cuda_environment()

from utils.data_utils import load_im, get_bbox, get_point_cloud_from_depth, get_resize_rgb_choose
from utils.draw_utils import draw_detections

import pycocotools.mask as cocomask
import trimesh

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.join(BASE_DIR, '..', 'Pose_Estimation_Model')
sys.path.append(os.path.join(ROOT_DIR, 'provider'))
sys.path.append(os.path.join(ROOT_DIR, 'utils'))
sys.path.append(os.path.join(ROOT_DIR, 'model'))
sys.path.append(os.path.join(BASE_DIR, 'model', 'pointnet2'))

def get_parser():
    parser = argparse.ArgumentParser(description="[OpenVINO] Pose Estimation (CPU Version)")
    # pem
    parser.add_argument("--device", type=str, default="CPU", help="device to run on (CPU/GPU)")
    parser.add_argument("--model" , type=str, default="pose_estimation_model", help="path to model file")
    parser.add_argument("--config", type=str, default="config/base.yaml", help="path to config file, different config.yaml use different config")
    parser.add_argument("--iter"  , type=int, default=600000, help="epoch num. for testing")
    parser.add_argument("--exp_id", type=int, default=0, help="")
    
    # Set default input parameter, reference demo.sh
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
    default_cad_path = os.path.join(project_root, 'SAM-6D/Data/Example/obj_000005.ply')
    default_rgb_path = os.path.join(project_root, 'SAM-6D/Data/Example/rgb.png')
    default_depth_path = os.path.join(project_root, 'SAM-6D/Data/Example/depth.png')
    default_camera_path = os.path.join(project_root, 'SAM-6D/Data/Example/camera.json')
    default_output_dir = os.path.join(project_root, 'SAM-6D/Data/Example/outputs')
    default_seg_path = os.path.join(default_output_dir, 'sam6d_results/detection_ism.json')

    parser.add_argument("--output_dir", nargs="?", default=default_output_dir, help="Path to root directory of the output")
    parser.add_argument("--cad_path", nargs="?", default=default_cad_path, help="Path to CAD(mm)")
    parser.add_argument("--rgb_path", nargs="?", default=default_rgb_path, help="Path to RGB image")
    parser.add_argument("--depth_path", nargs="?", default=default_depth_path, help="Path to Depth image(mm)")
    parser.add_argument("--cam_path", nargs="?", default=default_camera_path, help="Path to camera information")
    parser.add_argument("--seg_path", nargs="?", default=default_seg_path, help="Path to segmentation information(generated by ISM)")
    parser.add_argument("--det_score_thresh", default=0.2, help="The score threshold of detection")
    args_cfg = parser.parse_args()

    return args_cfg

# Config Init
def init():
    args = get_parser()
    exp_name = args.model + '_' + \
        osp.splitext(args.config.split("/")[-1])[0] + '_id' + str(args.exp_id)
    log_dir = osp.join("log", exp_name)

    # Load config directly from YAML file
    import yaml
    with open(args.config, 'r') as f:
        config_data = yaml.safe_load(f)
    
    # Create config object with all attributes
    class Config:
        def __init__(self, data):
            for key, value in data.items():
                if isinstance(value, dict):
                    setattr(self, key, Config(value))
                else:
                    setattr(self, key, value)
    
    cfg = Config(config_data)

    cfg.exp_name = exp_name
    cfg.device = args.device
    cfg.model_name = args.model
    cfg.log_dir = log_dir
    cfg.test_iter = args.iter

    cfg.output_dir = args.output_dir
    cfg.cad_path = args.cad_path
    cfg.rgb_path = args.rgb_path
    cfg.depth_path = args.depth_path
    cfg.cam_path = args.cam_path
    cfg.seg_path = args.seg_path

    cfg.det_score_thresh = args.det_score_thresh
    print(f"[OpenVINO] Using device: {cfg.device}")

    return cfg


def visualize(rgb, pred_rot, pred_trans, model_points, K, save_path):
    """
    Visualize the predicted pose by drawing the 3D model overlay on the RGB image.
    Args:
        rgb: np.ndarray, shape (H, W, 3), uint8, input RGB image
        pred_rot: np.ndarray, shape (N, 3, 3), predicted rotation matrices
        pred_trans: np.ndarray, shape (N, 3), predicted translations (mm)
        model_points: np.ndarray, shape (M, 3), 3D model points (mm)
        K: np.ndarray, shape (N, 3, 3), camera intrinsics
        save_path: str, path to save the visualization image
    Returns:
        concat: PIL.Image, side-by-side visualization image
    """
    img = draw_detections(rgb, pred_rot, pred_trans, model_points, K, color=(255, 0, 0))
    img = Image.fromarray(np.uint8(img))
    img.save(save_path)
    prediction = Image.open(save_path)
    
    # concat side by side in PIL
    rgb = Image.fromarray(np.uint8(rgb))
    img = np.array(img)
    concat = Image.new('RGB', (img.shape[1] + prediction.size[0], img.shape[0]))
    concat.paste(rgb, (0, 0))
    concat.paste(prediction, (img.shape[1], 0))
    return concat


def get_templates_np(path, cfg):
    """
    Load multiple rendered templates for the CAD model from disk using numpy.
    Args:
        path: str, directory containing template files
        cfg: config object, must have n_template_view, img_size, n_sample_template_point
    Returns:
        all_tem: list[np.ndarray], each (1, 3, img_size, img_size)
        all_tem_pts: list[np.ndarray], each (1, n_sample_template_point, 3)
        all_tem_choose: list[np.ndarray], each (1, n_sample_template_point)
    """
    n_template_view = cfg.n_template_view
    all_tem = []
    all_tem_choose = []
    all_tem_pts = []

    total_nView = 42
    for v in range(n_template_view):
        i = int(total_nView / n_template_view * v)
        tem, tem_choose, tem_pts = _get_template_np(path, cfg, i)
        all_tem.append(np.expand_dims(tem, axis=0))  # (1, 3, img_size, img_size)
        all_tem_choose.append(np.expand_dims(tem_choose, axis=0))  # (1, n_sample_template_point)
        all_tem_pts.append(np.expand_dims(tem_pts, axis=0))  # (1, n_sample_template_point, 3)
    return all_tem, all_tem_pts, all_tem_choose

def _get_template_np(path, cfg, tem_index=1):
    """
    Load a single template (rendered view) for the CAD model using numpy.
    Args:
        path: str, directory containing template files
        cfg: config object, must have img_size, n_sample_template_point, rgb_mask_flag
        tem_index: int, template index
    Returns:
        rgb: np.ndarray, shape (3, img_size, img_size), normalized RGB image
        rgb_choose: np.ndarray, shape (n_sample_template_point,), selected pixel indices
        xyz: np.ndarray, shape (n_sample_template_point, 3), 3D points (meters)
    """
    rgb_path = os.path.join(path, 'rgb_'+str(tem_index)+'.png')
    mask_path = os.path.join(path, 'mask_'+str(tem_index)+'.png')
    xyz_path = os.path.join(path, 'xyz_'+str(tem_index)+'.npy')

    # Load data using numpy/cv2
    rgb = cv2.imread(rgb_path, cv2.IMREAD_UNCHANGED).astype(np.uint8)
    xyz = np.load(xyz_path).astype(np.float32) / 1000.0  # Convert mm to meters
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.uint8) == 255

    # Get bounding box
    rows = np.any(mask, axis=1)
    cols = np.any(mask, axis=0)
    y1, y2 = np.where(rows)[0][[0, -1]]
    x1, x2 = np.where(cols)[0][[0, -1]]
    mask = mask[y1:y2, x1:x2]

    # Process RGB image
    rgb = rgb[:,:,::-1][y1:y2, x1:x2, :]  # BGR to RGB
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)

    # Resize RGB image
    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    
    # Normalize RGB (same as torchvision transforms)
    rgb = rgb.astype(np.float32) / 255.0
    rgb = (rgb - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
    rgb = rgb.transpose(2, 0, 1)  # (H, W, C) -> (C, H, W)

    # Process point cloud
    xyz = xyz[y1:y2, x1:x2, :].reshape((-1, 3))

    # Sample points
    choose = (mask>0).astype(np.float32).flatten().nonzero()[0]
    if len(choose) <= cfg.n_sample_template_point:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point)
    else:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=False)
    choose = choose[choose_idx]
    xyz = xyz[choose, :]

    # Calculate RGB choose indices
    h, w = y2 - y1, x2 - x1
    scale_h, scale_w = cfg.img_size / h, cfg.img_size / w
    
    choose_y = (choose // w).astype(np.float32) * scale_h
    choose_x = (choose % w).astype(np.float32) * scale_w
    rgb_choose = (choose_y * cfg.img_size + choose_x).astype(np.int32)

    return rgb, rgb_choose, xyz

def get_test_data_np(rgb_path, depth_path, cam_path, cad_path, seg_path, det_score_thresh, cfg):
    """
    Prepare test data for pose estimation using numpy.
    Args:
        rgb_path: str, path to RGB image
        depth_path: str, path to depth image
        cam_path: str, path to camera intrinsics (json)
        cad_path: str, path to CAD model
        seg_path: str, path to segmentation results (json)
        det_score_thresh: float, detection score threshold
        cfg: config object, must have n_sample_observed_point, img_size, rgb_mask_flag
    Returns:
        ret_dict: dict with keys:
            'pts': np.ndarray, (N, n_sample_observed_point, 3)
            'rgb': np.ndarray, (N, 3, img_size, img_size)
            'rgb_choose': np.ndarray, (N, n_sample_observed_point)
            'score': np.ndarray, (N,)
            'model': np.ndarray, (N, n_sample_model_point, 3)
            'K': np.ndarray, (N, 3, 3)
        whole_image: np.ndarray, (H, W, 3), original RGB image
        whole_pts: np.ndarray, (H*W, 3), full point cloud
        model_points: np.ndarray, (n_sample_model_point, 3)
        all_dets: list[dict], detection info
    """
    dets = []
    with open(seg_path) as f:
        dets_ = json.load(f) # keys: scene_id, image_id, category_id, bbox, score, segmentation
    for det in dets_:
        if det['score'] > det_score_thresh:
            dets.append(det)
    del dets_

    cam_info = json.load(open(cam_path))
    K = np.array(cam_info['cam_K']).reshape(3, 3)

    whole_image = load_im(rgb_path).astype(np.uint8)
    if len(whole_image.shape)==2:
        whole_image = np.concatenate([whole_image[:,:,None], whole_image[:,:,None], whole_image[:,:,None]], axis=2)
    whole_depth = load_im(depth_path).astype(np.float32) * cam_info['depth_scale'] / 1000.0
    whole_pts = get_point_cloud_from_depth(whole_depth, K)

    mesh = trimesh.load_mesh(cad_path)
    model_points = mesh.sample(cfg.n_sample_model_point).astype(np.float32) / 1000.0
    radius = np.max(np.linalg.norm(model_points, axis=1))

    all_rgb = []
    all_cloud = []
    all_rgb_choose = []
    all_score = []
    all_dets = []
    for inst in dets:
        seg = inst['segmentation']
        score = inst['score']

        # mask
        h,w = seg['size']
        try:
            rle = cocomask.frPyObjects(seg, h, w)
        except:
            rle = seg
        mask = cocomask.decode(rle)
        mask = np.logical_and(mask > 0, whole_depth > 0)
        if np.sum(mask) > 32:
            bbox = get_bbox(mask)
            y1, y2, x1, x2 = bbox
        else:
            continue
        mask = mask[y1:y2, x1:x2]
        choose = mask.astype(np.float32).flatten().nonzero()[0]

        # pts
        cloud = whole_pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)[choose, :]
        center = np.mean(cloud, axis=0)
        tmp_cloud = cloud - center[None, :]
        flag = np.linalg.norm(tmp_cloud, axis=1) < radius * 1.2
        if np.sum(flag) < 4:
            continue
        choose = choose[flag]
        cloud = cloud[flag]

        if len(choose) <= cfg.n_sample_observed_point:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point)
        else:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
        choose = choose[choose_idx]
        cloud = cloud[choose_idx]

        # rgb
        rgb = whole_image.copy()[y1:y2, x1:x2, :][:,:,::-1]
        if cfg.rgb_mask_flag:
            rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)
        rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
        
        # Normalize RGB (same as torchvision transforms)
        rgb = rgb.astype(np.float32) / 255.0
        rgb = (rgb - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
        rgb = rgb.transpose(2, 0, 1)  # (H, W, C) -> (C, H, W)
        
        rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)

        all_rgb.append(rgb.astype(np.float32))
        all_cloud.append(cloud.astype(np.float32))
        all_rgb_choose.append(rgb_choose.astype(np.int32))
        all_score.append(score)
        all_dets.append(inst)

    ret_dict = {}
    ret_dict['pts'] = np.stack(all_cloud)
    ret_dict['rgb'] = np.stack(all_rgb)
    ret_dict['rgb_choose'] = np.stack(all_rgb_choose)
    ret_dict['score'] = np.array(all_score, dtype=np.float32)

    ninstance = ret_dict['pts'].shape[0]
    ret_dict['model'] = np.repeat(model_points[np.newaxis, :, :], ninstance, axis=0)
    ret_dict['K'] = np.repeat(K[np.newaxis, :, :], ninstance, axis=0)
    return ret_dict, whole_image, whole_pts.reshape(-1, 3), model_points, all_dets


def main():
    # init config
    cfg = init()

    # ov init
    core = Core()
    ov_pem_model_path = "model_save/ov_pem_model_cpu.xml"
    ov_fe_model_path = "model_save/ov_fe_model_cpu.xml"
    ov_gpu_kernel_path = "./model/ov_pointnet2_op/ov_gpu_custom_op.xml"
    ov_extension_lib_path = "./model/ov_pointnet2_op/build/libopenvino_operation_extension.so"
    core.add_extension(ov_extension_lib_path)

    if cfg.device == "GPU":
        core.set_property("GPU", {"INFERENCE_PRECISION_HINT": "f32"})
        core.set_property("GPU", {"CONFIG_FILE": ov_gpu_kernel_path})

    # ov load models
    ov_pem_model = core.read_model(ov_pem_model_path)
    ov_fe_model = core.read_model(ov_fe_model_path)

    ov_fe_compiled_model = core.compile_model(ov_fe_model, cfg.device)
    ov_pem_compiled_model = core.compile_model(ov_pem_model, cfg.device)
    
    print("[OpenVINO] extracting templates ...")
    print("[OpenVINO] running fe model ...")
    tem_path = os.path.join(cfg.output_dir, 'templates')
    all_tem, all_tem_pts, all_tem_choose = get_templates_np(tem_path, cfg.test_dataset)
    
    # concat all templates(rgb, pts, choose)
    tem_rgb_concat = np.concatenate(all_tem, axis=1)  # (B, n_template_view*3, H, W)
    tem_pts_concat = np.concatenate(all_tem_pts, axis=1)  # (B, n_template_view*n_sample_template_point, 3)
    tem_choose_concat = np.concatenate(all_tem_choose, axis=1)  # (B, n_template_view*n_sample_template_point)
    
    # prepare OpenVINO input
    feature_inputs = {
        "rgb_input": tem_rgb_concat,
        "pts_input": tem_pts_concat,
        "choose_input": tem_choose_concat
    }

    # Warm up
    # feature_results = ov_fe_compiled_model(feature_inputs)

    # OV feature extraction inference
    time_start = time.time()
    feature_results = ov_fe_compiled_model(feature_inputs)
    results_list = list(feature_results.values())
    fe_time = time.time() - time_start
    print(f"[OpenVINO] fe (feature extraction) inference time: {fe_time*1000:.2f} ms")
    
    # get output
    all_tem_pts = results_list[0]  # tem_pts_out
    all_tem_feat = results_list[1]  # tem_feat

    # OV pose estimation inference
    print("[OpenVINO] loading pem input data ...")
    input_data, img, whole_pts, model_points, detections = get_test_data_np(
        cfg.rgb_path, cfg.depth_path, cfg.cam_path, cfg.cad_path, cfg.seg_path, 
        cfg.det_score_thresh, cfg.test_dataset
    )
    ninstance = input_data['pts'].shape[0]
    input_data['dense_po'] = np.repeat(all_tem_pts, ninstance, axis=0)
    input_data['dense_fo'] = np.repeat(all_tem_feat, ninstance, axis=0)
    
    ov_pem_inputs = {
            "pts": input_data['pts'],
            "rgb": input_data['rgb'],
            "rgb_choose": input_data['rgb_choose'],
            "model": input_data['model'],
            "dense_po": input_data['dense_po'],
            "dense_fo": input_data['dense_fo'],
        }

    # Warm up
    # results = ov_pem_compiled_model(ov_pem_inputs)

    # OV pose estimation inference
    print("[OpenVINO] running pem model ...")
    pem_time_start = time.time()
    results = ov_pem_compiled_model(ov_pem_inputs)
    pem_time = time.time() - pem_time_start
    print(f"[OpenVINO] pose estimation inference time: {pem_time*1000:.2f} ms")
    
    results_output = list(results.values())
    ov_pred_R = results_output[0]
    ov_pred_t = results_output[1]
    ov_pred_pose_score = results_output[2]

    pose_scores = ov_pred_pose_score * input_data['score']
    pred_rot = ov_pred_R
    pred_trans = ov_pred_t * 1000 

    # Save results
    print("[OpenVINO] saving results ...")
    for idx, det in enumerate(detections):
        detections[idx]['score'] = float(pose_scores[idx])
        detections[idx]['R'] = list(pred_rot[idx].tolist())
        detections[idx]['t'] = list(pred_trans[idx].tolist())

    with open(os.path.join(f"{cfg.output_dir}/sam6d_results", f'detection_pem_ov_{cfg.device}.json'), "w") as f:
        json.dump(detections, f)

    print("[OpenVINO] visualizating ...")
    save_path = os.path.join(f"{cfg.output_dir}/sam6d_results", f'vis_pem_ov_{cfg.device}.png')
    valid_masks = pose_scores == pose_scores.max()
    K = input_data['K'][valid_masks]
    vis_img = visualize(img, pred_rot[valid_masks], pred_trans[valid_masks], model_points*1000, K, save_path)
    vis_img.save(save_path)
    print(f"[OpenVINO Inference Done] Pose_Estimation_Model ({cfg.device} Version)") 
    print(f"[OpenVINO] PEM E2E Inference Time: {(fe_time + pem_time):.2f} s")    


if __name__ == "__main__":
    main()