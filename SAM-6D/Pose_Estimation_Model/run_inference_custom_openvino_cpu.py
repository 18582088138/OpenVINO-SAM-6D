import gorilla
import argparse
import os
import sys
from PIL import Image
import os.path as osp
import numpy as np
import time
import json
import cv2

from openvino import Core

import pycocotools.mask as cocomask
import trimesh

from utils.data_utils import load_im, get_bbox, get_point_cloud_from_depth, get_resize_rgb_choose
from utils.draw_utils import draw_detections

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.join(BASE_DIR, '..', 'Pose_Estimation_Model')
sys.path.append(os.path.join(ROOT_DIR, 'provider'))
sys.path.append(os.path.join(ROOT_DIR, 'utils'))
sys.path.append(os.path.join(ROOT_DIR, 'model'))
sys.path.append(os.path.join(BASE_DIR, 'model', 'pointnet2'))

def get_parser():
    parser = argparse.ArgumentParser(description="Pose Estimation Model Convert to ONNX and OpenVINO (CPU Version)")
    # pem
    parser.add_argument("--device", type=str, default="cpu", help="device to run on (cpu/cuda)")
    parser.add_argument("--model" , type=str, default="pose_estimation_model", help="path to model file")
    parser.add_argument("--config", type=str, default="config/base.yaml", help="path to config file, different config.yaml use different config")
    parser.add_argument("--iter"  , type=int, default=600000, help="epoch num. for testing")
    parser.add_argument("--exp_id", type=int, default=0, help="")
    
    # Set default input parameter, reference demo.sh
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
    default_cad_path = os.path.join(project_root, 'SAM-6D/Data/Example/obj_000005.ply')
    default_rgb_path = os.path.join(project_root, 'SAM-6D/Data/Example/rgb.png')
    default_depth_path = os.path.join(project_root, 'SAM-6D/Data/Example/depth.png')
    default_camera_path = os.path.join(project_root, 'SAM-6D/Data/Example/camera.json')
    default_output_dir = os.path.join(project_root, 'SAM-6D/Data/Example/outputs')
    default_seg_path = os.path.join(default_output_dir, 'sam6d_results/detection_ism.json')

    parser.add_argument("--output_dir", nargs="?", default=default_output_dir, help="Path to root directory of the output")
    parser.add_argument("--cad_path", nargs="?", default=default_cad_path, help="Path to CAD(mm)")
    parser.add_argument("--rgb_path", nargs="?", default=default_rgb_path, help="Path to RGB image")
    parser.add_argument("--depth_path", nargs="?", default=default_depth_path, help="Path to Depth image(mm)")
    parser.add_argument("--cam_path", nargs="?", default=default_camera_path, help="Path to camera information")
    parser.add_argument("--seg_path", nargs="?", default=default_seg_path, help="Path to segmentation information(generated by ISM)")
    parser.add_argument("--det_score_thresh", default=0.2, help="The score threshold of detection")
    args_cfg = parser.parse_args()

    return args_cfg

# Config Init
def init():
    args = get_parser()
    exp_name = args.model + '_' + \
        osp.splitext(args.config.split("/")[-1])[0] + '_id' + str(args.exp_id)
    log_dir = osp.join("log", exp_name)

    cfg = gorilla.Config.fromfile(args.config)
    cfg.exp_name = exp_name
    cfg.device = args.device
    cfg.model_name = args.model
    cfg.log_dir = log_dir
    cfg.test_iter = args.iter

    cfg.output_dir = args.output_dir
    cfg.cad_path = args.cad_path
    cfg.rgb_path = args.rgb_path
    cfg.depth_path = args.depth_path
    cfg.cam_path = args.cam_path
    cfg.seg_path = args.seg_path

    cfg.det_score_thresh = args.det_score_thresh
    print(f"Using device: {cfg.device}")

    return cfg


def visualize(rgb, pred_rot, pred_trans, model_points, K, save_path):
    """
    Visualize the predicted pose by drawing the 3D model overlay on the RGB image.
    Args:
        rgb: np.ndarray, shape (H, W, 3), uint8, input RGB image
        pred_rot: np.ndarray, shape (N, 3, 3), predicted rotation matrices
        pred_trans: np.ndarray, shape (N, 3), predicted translations (mm)
        model_points: np.ndarray, shape (M, 3), 3D model points (mm)
        K: np.ndarray, shape (N, 3, 3), camera intrinsics
        save_path: str, path to save the visualization image
    Returns:
        concat: PIL.Image, side-by-side visualization image
    """
    img = draw_detections(rgb, pred_rot, pred_trans, model_points, K, color=(255, 0, 0))
    img = Image.fromarray(np.uint8(img))
    img.save(save_path)
    prediction = Image.open(save_path)
    
    # concat side by side in PIL
    rgb = Image.fromarray(np.uint8(rgb))
    img = np.array(img)
    concat = Image.new('RGB', (img.shape[1] + prediction.size[0], img.shape[0]))
    concat.paste(rgb, (0, 0))
    concat.paste(prediction, (img.shape[1], 0))
    return concat


def get_templates_np(path, cfg):
    """
    Load multiple rendered templates for the CAD model from disk using numpy.
    Args:
        path: str, directory containing template files
        cfg: config object, must have n_template_view, img_size, n_sample_template_point
    Returns:
        all_tem: list[np.ndarray], each (1, 3, img_size, img_size)
        all_tem_pts: list[np.ndarray], each (1, n_sample_template_point, 3)
        all_tem_choose: list[np.ndarray], each (1, n_sample_template_point)
    """
    n_template_view = cfg.n_template_view
    all_tem = []
    all_tem_choose = []
    all_tem_pts = []

    total_nView = 42
    for v in range(n_template_view):
        i = int(total_nView / n_template_view * v)
        tem, tem_choose, tem_pts = _get_template_np(path, cfg, i)
        all_tem.append(np.expand_dims(tem, axis=0))  # (1, 3, img_size, img_size)
        all_tem_choose.append(np.expand_dims(tem_choose, axis=0))  # (1, n_sample_template_point)
        all_tem_pts.append(np.expand_dims(tem_pts, axis=0))  # (1, n_sample_template_point, 3)
    return all_tem, all_tem_pts, all_tem_choose

def _get_template_np(path, cfg, tem_index=1):
    """
    Load a single template (rendered view) for the CAD model using numpy.
    Args:
        path: str, directory containing template files
        cfg: config object, must have img_size, n_sample_template_point, rgb_mask_flag
        tem_index: int, template index
    Returns:
        rgb: np.ndarray, shape (3, img_size, img_size), normalized RGB image
        rgb_choose: np.ndarray, shape (n_sample_template_point,), selected pixel indices
        xyz: np.ndarray, shape (n_sample_template_point, 3), 3D points (meters)
    """
    rgb_path = os.path.join(path, 'rgb_'+str(tem_index)+'.png')
    mask_path = os.path.join(path, 'mask_'+str(tem_index)+'.png')
    xyz_path = os.path.join(path, 'xyz_'+str(tem_index)+'.npy')

    # Load data using numpy/cv2
    rgb = cv2.imread(rgb_path, cv2.IMREAD_UNCHANGED).astype(np.uint8)
    xyz = np.load(xyz_path).astype(np.float32) / 1000.0  # Convert mm to meters
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.uint8) == 255

    # Get bounding box
    rows = np.any(mask, axis=1)
    cols = np.any(mask, axis=0)
    y1, y2 = np.where(rows)[0][[0, -1]]
    x1, x2 = np.where(cols)[0][[0, -1]]
    mask = mask[y1:y2, x1:x2]

    # Process RGB image
    rgb = rgb[:,:,::-1][y1:y2, x1:x2, :]  # BGR to RGB
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)

    # Resize RGB image
    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    
    # Normalize RGB (same as torchvision transforms)
    rgb = rgb.astype(np.float32) / 255.0
    rgb = (rgb - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
    rgb = rgb.transpose(2, 0, 1)  # (H, W, C) -> (C, H, W)

    # Process point cloud
    xyz = xyz[y1:y2, x1:x2, :].reshape((-1, 3))

    # Sample points
    choose = (mask>0).astype(np.float32).flatten().nonzero()[0]
    if len(choose) <= cfg.n_sample_template_point:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point)
    else:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=False)
    choose = choose[choose_idx]
    xyz = xyz[choose, :]

    # Calculate RGB choose indices
    h, w = y2 - y1, x2 - x1
    scale_h, scale_w = cfg.img_size / h, cfg.img_size / w
    
    choose_y = (choose // w).astype(np.float32) * scale_h
    choose_x = (choose % w).astype(np.float32) * scale_w
    rgb_choose = (choose_y * cfg.img_size + choose_x).astype(np.int32)

    return rgb, rgb_choose, xyz

def get_test_data_np(rgb_path, depth_path, cam_path, cad_path, seg_path, det_score_thresh, cfg):
    """
    Prepare test data for pose estimation using numpy.
    Args:
        rgb_path: str, path to RGB image
        depth_path: str, path to depth image
        cam_path: str, path to camera intrinsics (json)
        cad_path: str, path to CAD model
        seg_path: str, path to segmentation results (json)
        det_score_thresh: float, detection score threshold
        cfg: config object, must have n_sample_observed_point, img_size, rgb_mask_flag
    Returns:
        ret_dict: dict with keys:
            'pts': np.ndarray, (N, n_sample_observed_point, 3)
            'rgb': np.ndarray, (N, 3, img_size, img_size)
            'rgb_choose': np.ndarray, (N, n_sample_observed_point)
            'score': np.ndarray, (N,)
            'model': np.ndarray, (N, n_sample_model_point, 3)
            'K': np.ndarray, (N, 3, 3)
        whole_image: np.ndarray, (H, W, 3), original RGB image
        whole_pts: np.ndarray, (H*W, 3), full point cloud
        model_points: np.ndarray, (n_sample_model_point, 3)
        all_dets: list[dict], detection info
    """
    dets = []
    with open(seg_path) as f:
        dets_ = json.load(f) # keys: scene_id, image_id, category_id, bbox, score, segmentation
    for det in dets_:
        if det['score'] > det_score_thresh:
            dets.append(det)
    del dets_

    cam_info = json.load(open(cam_path))
    K = np.array(cam_info['cam_K']).reshape(3, 3)

    whole_image = load_im(rgb_path).astype(np.uint8)
    if len(whole_image.shape)==2:
        whole_image = np.concatenate([whole_image[:,:,None], whole_image[:,:,None], whole_image[:,:,None]], axis=2)
    whole_depth = load_im(depth_path).astype(np.float32) * cam_info['depth_scale'] / 1000.0
    whole_pts = get_point_cloud_from_depth(whole_depth, K)

    mesh = trimesh.load_mesh(cad_path)
    model_points = mesh.sample(cfg.n_sample_model_point).astype(np.float32) / 1000.0
    radius = np.max(np.linalg.norm(model_points, axis=1))

    all_rgb = []
    all_cloud = []
    all_rgb_choose = []
    all_score = []
    all_dets = []
    for inst in dets:
        seg = inst['segmentation']
        score = inst['score']

        # mask
        h,w = seg['size']
        try:
            rle = cocomask.frPyObjects(seg, h, w)
        except:
            rle = seg
        mask = cocomask.decode(rle)
        mask = np.logical_and(mask > 0, whole_depth > 0)
        if np.sum(mask) > 32:
            bbox = get_bbox(mask)
            y1, y2, x1, x2 = bbox
        else:
            continue
        mask = mask[y1:y2, x1:x2]
        choose = mask.astype(np.float32).flatten().nonzero()[0]

        # pts
        cloud = whole_pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)[choose, :]
        center = np.mean(cloud, axis=0)
        tmp_cloud = cloud - center[None, :]
        flag = np.linalg.norm(tmp_cloud, axis=1) < radius * 1.2
        if np.sum(flag) < 4:
            continue
        choose = choose[flag]
        cloud = cloud[flag]

        if len(choose) <= cfg.n_sample_observed_point:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point)
        else:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
        choose = choose[choose_idx]
        cloud = cloud[choose_idx]

        # rgb
        rgb = whole_image.copy()[y1:y2, x1:x2, :][:,:,::-1]
        if cfg.rgb_mask_flag:
            rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)
        rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
        
        # Normalize RGB (same as torchvision transforms)
        rgb = rgb.astype(np.float32) / 255.0
        rgb = (rgb - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
        rgb = rgb.transpose(2, 0, 1)  # (H, W, C) -> (C, H, W)
        
        rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)

        all_rgb.append(rgb.astype(np.float32))
        all_cloud.append(cloud.astype(np.float32))
        all_rgb_choose.append(rgb_choose.astype(np.int32))
        all_score.append(score)
        all_dets.append(inst)

    ret_dict = {}
    ret_dict['pts'] = np.stack(all_cloud)
    ret_dict['rgb'] = np.stack(all_rgb)
    ret_dict['rgb_choose'] = np.stack(all_rgb_choose)
    ret_dict['score'] = np.array(all_score, dtype=np.float32)

    ninstance = ret_dict['pts'].shape[0]
    ret_dict['model'] = np.repeat(model_points[np.newaxis, :, :], ninstance, axis=0)
    ret_dict['K'] = np.repeat(K[np.newaxis, :, :], ninstance, axis=0)
    return ret_dict, whole_image, whole_pts.reshape(-1, 3), model_points, all_dets


def main():
    cfg = init()
    core = Core()

    ov_pem_model_path = "model_save/ov_pem_model_cpu.xml"
    ov_fe_model_path = "model_save/ov_fe_model_cpu.xml"
    ov_extension_lib_path = "./model/ov_pointnet2_op/build/libopenvino_operation_extension.so"
    core.add_extension(ov_extension_lib_path)


    ov_pem_model = core.read_model(ov_pem_model_path)
    ov_fe_model = core.read_model(ov_fe_model_path)

    ov_pem_compiled_model = core.compile_model(ov_pem_model, 'CPU')
    ov_fe_compiled_model = core.compile_model(ov_fe_model, 'CPU')

    # set device
    device = cfg.device
    print(f"Using device: {device}")

    print("=> extracting templates ...")
    tem_path = os.path.join(cfg.output_dir, 'templates')
    # all_tem, all_tem_pts, all_tem_choose = get_templates(tem_path, cfg.test_dataset, device)
    
    all_tem, all_tem_pts, all_tem_choose = get_templates_np(tem_path, cfg.test_dataset)
    
    # OV feature extraction inference
    # 拼接RGB张量
    tem_rgb_concat = np.concatenate(all_tem, axis=1)  # (B, n_template_view*3, H, W)
    
    # 拼接点云张量
    tem_pts_concat = np.concatenate(all_tem_pts, axis=1)  # (B, n_template_view*n_sample_template_point, 3)
    
    # 拼接选择索引张量
    tem_choose_concat = np.concatenate(all_tem_choose, axis=1)  # (B, n_template_view*n_sample_template_point)
    
    # prepare OpenVINO input
    feature_inputs = {
        "rgb_input": tem_rgb_concat,
        "pts_input": tem_pts_concat,
        "choose_input": tem_choose_concat
    }
    
    # OV inference
    feature_results = ov_fe_compiled_model(feature_inputs)
    results_list = list(feature_results.values())
    
    # get output
    all_tem_pts = results_list[0]  # tem_pts_out
    all_tem_feat = results_list[1]  # tem_feat

    # OV pose estimation inference
    print("=> loading input data ...")
    input_data, img, whole_pts, model_points, detections = get_test_data_np(
        cfg.rgb_path, cfg.depth_path, cfg.cam_path, cfg.cad_path, cfg.seg_path, 
        cfg.det_score_thresh, cfg.test_dataset
    )
    ninstance = input_data['pts'].shape[0]
    input_data['dense_po'] = np.repeat(all_tem_pts, ninstance, axis=0)
    input_data['dense_fo'] = np.repeat(all_tem_feat, ninstance, axis=0)
    
    ov_pem_inputs = {
            "pts": input_data['pts'],
            "rgb": input_data['rgb'],
            "rgb_choose": input_data['rgb_choose'],
            "model": input_data['model'],
            "dense_po": input_data['dense_po'],
            "dense_fo": input_data['dense_fo'],
        }
    # 推理
    print("=>[OpenVINO] running model ...")
    results = ov_pem_compiled_model(ov_pem_inputs)

    results_output = list(results.values())
    ov_pred_R = results_output[0]
    ov_pred_t = results_output[1]
    ov_pred_pose_score = results_output[2]

    pose_scores = ov_pred_pose_score * input_data['score']
    pred_rot = ov_pred_R
    pred_trans = ov_pred_t * 1000 

    # Save results
    print("=> saving results ...")
    for idx, det in enumerate(detections):
        detections[idx]['score'] = float(pose_scores[idx])
        detections[idx]['R'] = list(pred_rot[idx].tolist())
        detections[idx]['t'] = list(pred_trans[idx].tolist())

    with open(os.path.join(f"{cfg.output_dir}/sam6d_results", f'detection_pem_ov_{cfg.device}.json'), "w") as f:
        json.dump(detections, f)

    print("=> visualizating ...")
    save_path = os.path.join(f"{cfg.output_dir}/sam6d_results", f'vis_pem_ov_{cfg.device}.png')
    valid_masks = pose_scores == pose_scores.max()
    K = input_data['K'][valid_masks]
    vis_img = visualize(img, pred_rot[valid_masks], pred_trans[valid_masks], model_points*1000, K, save_path)
    vis_img.save(save_path)
    print(f"[OpenVINO Inference Done] Pose_Estimation_Model ({cfg.device} Version)") 


if __name__ == "__main__":
    main()