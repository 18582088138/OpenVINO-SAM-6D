import gorilla
import argparse
import os
import sys
from PIL import Image
import os.path as osp
import numpy as np
import random
import importlib
import json

import torch
import torchvision.transforms as transforms
import cv2

import torch.nn as nn
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.join(BASE_DIR, '..', 'Pose_Estimation_Model')
sys.path.append(os.path.join(ROOT_DIR, 'provider'))
sys.path.append(os.path.join(ROOT_DIR, 'utils'))
sys.path.append(os.path.join(ROOT_DIR, 'model'))
sys.path.append(os.path.join(BASE_DIR, 'model', 'pointnet2'))


def get_parser():
    parser = argparse.ArgumentParser(
        description="Pose Estimation Model Convert (CPU Version)")
    # pem
    parser.add_argument("--device",
                        type=str,
                        default="cpu",
                        help="device to run on (cpu/cuda)")
    parser.add_argument("--model",
                        type=str,
                        default="pose_estimation_model",
                        help="path to model file")
    parser.add_argument("--config",
                        type=str,
                        default="config/base.yaml",
                        help="path to config file, different config.yaml use different config")
    parser.add_argument("--iter",
                        type=int,
                        default=600000,
                        help="epoch num. for testing")
    parser.add_argument("--exp_id",
                        type=int,
                        default=0,
                        help="")
    
    # Set default input parameter, reference demo.sh
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
    default_cad_path = os.path.join(project_root, 'SAM-6D/Data/Example/obj_000005.ply')
    default_rgb_path = os.path.join(project_root, 'SAM-6D/Data/Example/rgb.png')
    default_depth_path = os.path.join(project_root, 'SAM-6D/Data/Example/depth.png')
    default_camera_path = os.path.join(project_root, 'SAM-6D/Data/Example/camera.json')
    default_output_dir = os.path.join(project_root, 'SAM-6D/Data/Example/outputs')
    default_seg_path = os.path.join(default_output_dir, 'sam6d_results/detection_ism.json')

    parser.add_argument("--output_dir", nargs="?", default=default_output_dir, help="Path to root directory of the output")
    parser.add_argument("--cad_path", nargs="?", default=default_cad_path, help="Path to CAD(mm)")
    parser.add_argument("--rgb_path", nargs="?", default=default_rgb_path, help="Path to RGB image")
    parser.add_argument("--depth_path", nargs="?", default=default_depth_path, help="Path to Depth image(mm)")
    parser.add_argument("--cam_path", nargs="?", default=default_camera_path, help="Path to camera information")
    parser.add_argument("--seg_path", nargs="?", default=default_seg_path, help="Path to segmentation information(generated by ISM)")
    parser.add_argument("--det_score_thresh", default=0.2, help="The score threshold of detection")
    args_cfg = parser.parse_args()

    return args_cfg

def init():
    args = get_parser()
    exp_name = args.model + '_' + \
        osp.splitext(args.config.split("/")[-1])[0] + '_id' + str(args.exp_id)
    log_dir = osp.join("log", exp_name)

    cfg = gorilla.Config.fromfile(args.config)
    cfg.exp_name = exp_name
    cfg.device = args.device
    cfg.model_name = args.model
    cfg.log_dir = log_dir
    cfg.test_iter = args.iter

    cfg.output_dir = args.output_dir
    cfg.cad_path = args.cad_path
    cfg.rgb_path = args.rgb_path
    cfg.depth_path = args.depth_path
    cfg.cam_path = args.cam_path
    cfg.seg_path = args.seg_path

    cfg.det_score_thresh = args.det_score_thresh
    
    # 检查设备可用性
    if cfg.device == "cuda" and not torch.cuda.is_available():
        print("CUDA不可用，自动切换到CPU")
        cfg.device = "cpu"
    
    print(f"使用设备: {cfg.device}")

    return cfg


# 尝试导入依赖模块，如果不存在则提供替代实现
try:
    from data_utils import (
        load_im,
        get_bbox,
        get_point_cloud_from_depth,
        get_resize_rgb_choose,
    )
    from draw_utils import draw_detections
except ImportError:
    print("警告: 无法导入data_utils或draw_utils，使用简化实现")
    
    def load_im(path):
        """加载图像文件"""
        if path.endswith('.png') or path.endswith('.jpg') or path.endswith('.jpeg'):
            return cv2.imread(path, cv2.IMREAD_UNCHANGED)
        else:
            return np.load(path)
    
    def get_bbox(mask):
        """获取mask的边界框"""
        rows = np.any(mask, axis=1)
        cols = np.any(mask, axis=0)
        y1, y2 = np.where(rows)[0][[0, -1]]
        x1, x2 = np.where(cols)[0][[0, -1]]
        return y1, y2, x1, x2
    
    def get_point_cloud_from_depth(depth, K):
        """从深度图生成点云"""
        h, w = depth.shape
        fx, fy = K[0, 0], K[1, 1]
        cx, cy = K[0, 2], K[1, 2]
        
        y, x = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')
        x = (x - cx) * depth / fx
        y = (y - cy) * depth / fy
        z = depth
        
        return np.stack([x, y, z], axis=-1)
    
    def get_resize_rgb_choose(choose, bbox, img_size):
        """获取调整大小后的RGB选择索引"""
        y1, y2, x1, x2 = bbox
        h, w = y2 - y1, x2 - x1
        scale_h, scale_w = img_size / h, img_size / w
        
        choose_y = (choose // w).astype(np.float32) * scale_h
        choose_x = (choose % w).astype(np.float32) * scale_w
        choose = choose_y * img_size + choose_x
        return choose.astype(np.int32)
    
    def draw_detections(rgb, pred_rot, pred_trans, model_points, K, color=(255, 0, 0)):
        """绘制检测结果（简化版本）"""
        # 简化实现，只返回原图
        return rgb.copy()

import pycocotools.mask as cocomask
import trimesh

rgb_transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                    std=[0.229, 0.224, 0.225])])

def visualize(rgb, pred_rot, pred_trans, model_points, K, save_path):
    """
    Visualize the predicted pose by drawing the 3D model overlay on the RGB image.
    Args:
        rgb: np.ndarray, shape (H, W, 3), uint8, input RGB image
        pred_rot: np.ndarray, shape (N, 3, 3), predicted rotation matrices
        pred_trans: np.ndarray, shape (N, 3), predicted translations (mm)
        model_points: np.ndarray, shape (M, 3), 3D model points (mm)
        K: np.ndarray, shape (N, 3, 3), camera intrinsics
        save_path: str, path to save the visualization image
    Returns:
        concat: PIL.Image, side-by-side visualization image
    """
    img = draw_detections(rgb, pred_rot, pred_trans, model_points, K, color=(255, 0, 0))
    img = Image.fromarray(np.uint8(img))
    img.save(save_path)
    prediction = Image.open(save_path)
    
    # concat side by side in PIL
    rgb = Image.fromarray(np.uint8(rgb))
    img = np.array(img)
    concat = Image.new('RGB', (img.shape[1] + prediction.size[0], img.shape[0]))
    concat.paste(rgb, (0, 0))
    concat.paste(prediction, (img.shape[1], 0))
    return concat


def _get_template(path, cfg, device, tem_index=1):
    """
    Load a single template (rendered view) for the CAD model.
    Args:
        path: str, directory containing template files
        cfg: config object, must have img_size, n_sample_template_point, rgb_mask_flag
        device: str, device to place tensors on
        tem_index: int, template index
    Returns:
        rgb: torch.Tensor, shape (3, img_size, img_size), normalized RGB image
        rgb_choose: torch.Tensor, shape (n_sample_template_point,), selected pixel indices
        xyz: np.ndarray, shape (n_sample_template_point, 3), 3D points (meters)
    """
    rgb_path = os.path.join(path, 'rgb_'+str(tem_index)+'.png')
    mask_path = os.path.join(path, 'mask_'+str(tem_index)+'.png')
    xyz_path = os.path.join(path, 'xyz_'+str(tem_index)+'.npy')

    rgb = load_im(rgb_path).astype(np.uint8)
    xyz = np.load(xyz_path).astype(np.float32) / 1000.0  
    mask = load_im(mask_path).astype(np.uint8) == 255

    bbox = get_bbox(mask)
    y1, y2, x1, x2 = bbox
    mask = mask[y1:y2, x1:x2]

    rgb = rgb[:,:,::-1][y1:y2, x1:x2, :]
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)

    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    rgb = rgb_transform(np.array(rgb))

    choose = (mask>0).astype(np.float32).flatten().nonzero()[0]
    if len(choose) <= cfg.n_sample_template_point:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point)
    else:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=False)
    choose = choose[choose_idx]
    xyz = xyz[y1:y2, x1:x2, :].reshape((-1, 3))[choose, :]

    rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)
    return rgb, rgb_choose, xyz


def get_templates(path, cfg, device):
    """
    Load multiple rendered templates for the CAD model from disk.
    Args:
        path: str, directory containing template files
        cfg: config object, must have n_template_view, img_size, n_sample_template_point
        device: str, device to place tensors on
    Returns:
        all_tem: list[torch.Tensor], each (1, 3, img_size, img_size)
        all_tem_pts: list[torch.Tensor], each (1, n_sample_template_point, 3)
        all_tem_choose: list[torch.Tensor], each (1, n_sample_template_point)
    """
    n_template_view = cfg.n_template_view
    all_tem = []
    all_tem_choose = []
    all_tem_pts = []

    total_nView = 42
    for v in range(n_template_view):
        i = int(total_nView / n_template_view * v)
        tem, tem_choose, tem_pts = _get_template(path, cfg, device, i)
        all_tem.append(torch.FloatTensor(tem).unsqueeze(0).to(device))
        all_tem_choose.append(torch.IntTensor(tem_choose).long().unsqueeze(0).to(device))
        all_tem_pts.append(torch.FloatTensor(tem_pts).unsqueeze(0).to(device))
    return all_tem, all_tem_pts, all_tem_choose


def get_test_data(rgb_path, depth_path, cam_path, cad_path, seg_path, det_score_thresh, cfg, device):
    """
    Prepare test data for pose estimation.
    Args:
        rgb_path: str, path to RGB image
        depth_path: str, path to depth image
        cam_path: str, path to camera intrinsics (json)
        cad_path: str, path to CAD model
        seg_path: str, path to segmentation results (json)
        det_score_thresh: float, detection score threshold
        cfg: config object, must have n_sample_observed_point, img_size, rgb_mask_flag
        device: str, device to place tensors on
    Returns:
        ret_dict: dict with keys:
            'pts': torch.Tensor, (N, n_sample_observed_point, 3)
            'rgb': torch.Tensor, (N, 3, img_size, img_size)
            'rgb_choose': torch.Tensor, (N, n_sample_observed_point)
            'score': torch.Tensor, (N,)
            'model': torch.Tensor, (N, n_sample_model_point, 3)
            'K': torch.Tensor, (N, 3, 3)
        whole_image: np.ndarray, (H, W, 3), original RGB image
        whole_pts: np.ndarray, (H*W, 3), full point cloud
        model_points: np.ndarray, (n_sample_model_point, 3)
        all_dets: list[dict], detection info
    """
    dets = []
    with open(seg_path) as f:
        dets_ = json.load(f) # keys: scene_id, image_id, category_id, bbox, score, segmentation
    for det in dets_:
        if det['score'] > det_score_thresh:
            dets.append(det)
    del dets_

    cam_info = json.load(open(cam_path))
    K = np.array(cam_info['cam_K']).reshape(3, 3)

    whole_image = load_im(rgb_path).astype(np.uint8)
    if len(whole_image.shape)==2:
        whole_image = np.concatenate([whole_image[:,:,None], whole_image[:,:,None], whole_image[:,:,None]], axis=2)
    whole_depth = load_im(depth_path).astype(np.float32) * cam_info['depth_scale'] / 1000.0
    whole_pts = get_point_cloud_from_depth(whole_depth, K)

    mesh = trimesh.load_mesh(cad_path)
    model_points = mesh.sample(cfg.n_sample_model_point).astype(np.float32) / 1000.0
    radius = np.max(np.linalg.norm(model_points, axis=1))

    all_rgb = []
    all_cloud = []
    all_rgb_choose = []
    all_score = []
    all_dets = []
    for inst in dets:
        seg = inst['segmentation']
        score = inst['score']

        # mask
        h,w = seg['size']
        try:
            rle = cocomask.frPyObjects(seg, h, w)
        except:
            rle = seg
        mask = cocomask.decode(rle)
        mask = np.logical_and(mask > 0, whole_depth > 0)
        if np.sum(mask) > 32:
            bbox = get_bbox(mask)
            y1, y2, x1, x2 = bbox
        else:
            continue
        mask = mask[y1:y2, x1:x2]
        choose = mask.astype(np.float32).flatten().nonzero()[0]

        # pts
        cloud = whole_pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)[choose, :]
        center = np.mean(cloud, axis=0)
        tmp_cloud = cloud - center[None, :]
        flag = np.linalg.norm(tmp_cloud, axis=1) < radius * 1.2
        if np.sum(flag) < 4:
            continue
        choose = choose[flag]
        cloud = cloud[flag]

        if len(choose) <= cfg.n_sample_observed_point:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point)
        else:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
        choose = choose[choose_idx]
        cloud = cloud[choose_idx]

        # rgb
        rgb = whole_image.copy()[y1:y2, x1:x2, :][:,:,::-1]
        if cfg.rgb_mask_flag:
            rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)
        rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
        rgb = rgb_transform(np.array(rgb))
        rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)

        all_rgb.append(torch.FloatTensor(rgb))
        all_cloud.append(torch.FloatTensor(cloud))
        all_rgb_choose.append(torch.IntTensor(rgb_choose).long())
        all_score.append(score)
        all_dets.append(inst)

    ret_dict = {}
    ret_dict['pts'] = torch.stack(all_cloud).to(device)
    ret_dict['rgb'] = torch.stack(all_rgb).to(device)
    ret_dict['rgb_choose'] = torch.stack(all_rgb_choose).to(device)
    ret_dict['score'] = torch.FloatTensor(all_score).to(device)

    ninstance = ret_dict['pts'].size(0)
    ret_dict['model'] = torch.FloatTensor(model_points).unsqueeze(0).repeat(ninstance, 1, 1).to(device)
    ret_dict['K'] = torch.FloatTensor(K).unsqueeze(0).repeat(ninstance, 1, 1).to(device)
    return ret_dict, whole_image, whole_pts.reshape(-1, 3), model_points, all_dets


class PEMWrapperModel(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.input_keys = [
            'pts', 'rgb', 'rgb_choose', 'score', 'model', 'K',
            'dense_po', 'dense_fo', 'init_R', 'init_t',
            'pred_R', 'pred_t', 'pred_pose_score'
        ]
        self.output_key = 'pred_t'

    def forward(self, pts, rgb, rgb_choose, score, model_pts, K,
                dense_po, dense_fo, init_R, init_t,
                pred_R, pred_t, pred_pose_score):
        inputs = {
            'pts': pts,
            'rgb': rgb,
            'rgb_choose': rgb_choose,
            'score': score,
            'model': model_pts,
            'K': K,
            'dense_po': dense_po,
            'dense_fo': dense_fo,
            'init_R': init_R,
            'init_t': init_t,
            'pred_R': pred_R,
            'pred_t': pred_t,
            'pred_pose_score': pred_pose_score,
        }
        return self.model(inputs)


def create_simplified_model_for_export(original_model, device):
    """
    创建一个简化的模型用于ONNX导出，避免自定义算子问题
    """
    class SimplifiedPEMModel(nn.Module):
        def __init__(self, original_model):
            super().__init__()
            self.original_model = original_model
            
            # 只保留可以导出的部分
            self.feature_extraction = original_model.feature_extraction
            
        def forward(self, pts, rgb, rgb_choose, score, model_pts, K,
                    dense_po, dense_fo, init_R, init_t,
                    pred_R, pred_t, pred_pose_score):
            """
            简化的前向传播，只包含基本操作
            """
            # 这里只实现基本的前向传播，避免自定义算子
            # 实际使用时需要根据具体模型结构调整
            
            # 示例：简单的特征提取和预测
            batch_size = pts.size(0)
            
            # 假设输出平移向量
            pred_translation = torch.zeros(batch_size, 3, device=pts.device)
            
            return pred_translation
    
    return SimplifiedPEMModel(original_model)


if __name__ == "__main__":
    cfg = init()

    random.seed(cfg.rd_seed)
    torch.manual_seed(cfg.rd_seed)

    # 设置设备
    device = torch.device(cfg.device)
    print(f"使用设备: {device}")

    # model
    print("=> creating model ...")
    MODEL = importlib.import_module(cfg.model_name)
    model = MODEL.Net(cfg.model)
    model = model.to(device)
    model.eval()
    checkpoint = os.path.join(os.path.dirname((os.path.abspath(__file__))), 'checkpoints', 'sam-6d-pem-base.pth')
    
    # 加载checkpoint时指定map_location
    gorilla.solver.load_checkpoint(model=model, filename=checkpoint, map_location=device)

    print("=> extracting templates ...")
    tem_path = os.path.join(cfg.output_dir, 'templates')
    all_tem, all_tem_pts, all_tem_choose = get_templates(tem_path, cfg.test_dataset, device)
    with torch.no_grad():
        all_tem_pts, all_tem_feat = model.feature_extraction.get_obj_feats(all_tem, all_tem_pts, all_tem_choose)

    print("=> loading input data ...")
    input_data, img, whole_pts, model_points, detections = get_test_data(
        cfg.rgb_path, cfg.depth_path, cfg.cam_path, cfg.cad_path, cfg.seg_path, 
        cfg.det_score_thresh, cfg.test_dataset, device
    )
    ninstance = input_data['pts'].size(0)
    
    print("=> running model ...")
    with torch.no_grad():
        input_data['dense_po'] = all_tem_pts.repeat(ninstance,1,1)
        input_data['dense_fo'] = all_tem_feat.repeat(ninstance,1,1)
        out = model(input_data)

    # 创建包装模型用于导出
    pem_wrapped_model = PEMWrapperModel(model).to(device).eval()

    # 准备示例输入
    example_inputs = tuple(input_data[k] for k in pem_wrapped_model.input_keys)

    onnx_model_path = "pose_estimation_model_cpu.onnx"

    print("=> 尝试ONNX导出...")
    try:
        torch.onnx.export(
            pem_wrapped_model,
            example_inputs,
            onnx_model_path,
            opset_version=20,
            operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK,
            input_names=pem_wrapped_model.input_keys,
            output_names=[pem_wrapped_model.output_key],
            dynamic_axes={k: {0: "batch"} for k in pem_wrapped_model.input_keys},
        )
        print(f"[ONNX] PEM ONNX model export success: {onnx_model_path}")
        
        # 尝试OpenVINO转换
        print("=> 尝试OpenVINO转换...")
        try:
            import openvino as ov
            from openvino import Core
            
            # ov_extension_lib_path = 'model/libopenvino_operation_extension.so'
            ov_extension_lib_path = '/home/intel/xkd/OpenVINO-SAM-6D/SAM-6D/Pose_Estimation_Model/model/ov_pointnet2_op/build/libopenvino_operation_extension.so'
            ov_model_path = "pose_estimation_model_cpu.xml"

            core = Core()
            
            core.add_extension(ov_extension_lib_path)

            ov_model = core.read_model(onnx_model_path)
            ov_compiled_model = core.compile_model(ov_model, 'CPU')
            
            ov.save_model(ov_model, ov_model_path)
            print(f"[OpenVINO] 模型转换成功: {ov_model_path}")
            
        except Exception as e:
            print(f"[OpenVINO] 转换失败: {e}")
            print("建议：")
            print("1. 检查ONNX模型是否包含不支持的算子")
            print("2. 考虑使用简化版本的模型")
            print("3. 或者直接使用PyTorch模型进行CPU推理")
            
    except Exception as e:
        print(f"[ONNX] 导出失败: {e}")
        print("原因分析：")
        print("1. 模型包含自定义CUDA扩展（BallQuery, GroupingOperation等）")
        print("2. 包含PyTorch特定的操作（org.pytorch.aten.ATen）")
        print("3. 某些层的输入形状无法确定")
        
        print("\n解决方案：")
        print("1. 使用CPU版本的推理脚本：run_inference_custom_cpu.py")
        print("2. 或者修改模型架构，移除自定义算子")
        print("3. 或者使用TorchScript进行模型优化")

    # 保存PyTorch推理结果
    if 'pred_pose_score' in out.keys():
        pose_scores = out['pred_pose_score'] * out['score']
    else:
        pose_scores = out['score']
    pose_scores = pose_scores.detach().cpu().numpy()
    pred_rot = out['pred_R'].detach().cpu().numpy()
    pred_trans = out['pred_t'].detach().cpu().numpy() * 1000

    print("=> saving results ...")
    os.makedirs(f"{cfg.output_dir}/sam6d_results", exist_ok=True)
    for idx, det in enumerate(detections):
        detections[idx]['score'] = float(pose_scores[idx])
        detections[idx]['R'] = list(pred_rot[idx].tolist())
        detections[idx]['t'] = list(pred_trans[idx].tolist())

    with open(os.path.join(f"{cfg.output_dir}/sam6d_results", 'detection_pem_cpu.json'), "w") as f:
        json.dump(detections, f)

    print("=> visualizating ...")
    save_path = os.path.join(f"{cfg.output_dir}/sam6d_results", 'vis_pem_cpu.png')
    valid_masks = pose_scores == pose_scores.max()
    K = input_data['K'].detach().cpu().numpy()[valid_masks]
    vis_img = visualize(img, pred_rot[valid_masks], pred_trans[valid_masks], model_points*1000, K, save_path)
    vis_img.save(save_path)
    print("[Inference Done] Pose_Estimation_Model (CPU Version)") 