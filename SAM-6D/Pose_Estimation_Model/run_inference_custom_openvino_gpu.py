# Suppress NVML errors when CUDA is not available
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# Environment setup for CUDA compatibility
def setup_cuda_environment():
    """Setup CUDA environment and handle NVML errors gracefully"""
    # Check CUDA availability
    cuda_available = torch.cuda.is_available()
    
    # Try to import gpustat but handle the case when NVML is not available
    gpustat_module = None
    try:
        import gpustat
        gpustat_module = gpustat
        if cuda_available:
            print("GPU monitoring enabled")
        else:
            print("CUDA not available, GPU monitoring disabled")
    except ImportError:
        print("Warning: gpustat not available, GPU monitoring disabled")
    except Exception as e:
        if "NVML" in str(e) or "nvidia" in str(e).lower():
            print("Warning: NVIDIA Management Library not available, GPU monitoring disabled")
        else:
            print(f"Warning: Error importing gpustat: {e}")
    
    return cuda_available, gpustat_module

# Try to import gorilla with error handling
gorilla_module = None
try:
    import gorilla
    gorilla_module = gorilla
    print("Gorilla library imported successfully")
except Exception as e:
    if "NVML" in str(e) or "nvidia" in str(e).lower():
        print("Warning: Gorilla library not available due to NVML error, using fallback configuration")
        gorilla_module = None
    else:
        print(f"Warning: Error importing gorilla: {e}")
        gorilla_module = None

import argparse
import os
import sys
import random
from PIL import Image
import os.path as osp
import numpy as np
import time
import json
import cv2

import torch
import torch.nn as nn
import torchvision.transforms as transforms

from openvino import Core

# Setup environment
CUDA_AVAILABLE, gpustat = setup_cuda_environment()

from utils.data_utils import load_im, get_bbox, get_point_cloud_from_depth, get_resize_rgb_choose
from utils.draw_utils import draw_detections

import pycocotools.mask as cocomask
import trimesh

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.join(BASE_DIR, '..', 'Pose_Estimation_Model')
sys.path.append(os.path.join(ROOT_DIR, 'provider'))
sys.path.append(os.path.join(ROOT_DIR, 'utils'))
sys.path.append(os.path.join(ROOT_DIR, 'model'))
sys.path.append(os.path.join(BASE_DIR, 'model', 'pointnet2'))

from utils.model_utils import compute_coarse_Rt, compute_fine_Rt
# from run_inference_custom_pytorch import *

def get_parser():
    parser = argparse.ArgumentParser(description="[OpenVINO] Pose Estimation (CPU Version)")
    # pem
    parser.add_argument("--device", type=str, default="CPU", help="device to run on (CPU/GPU)")
    parser.add_argument("--model" , type=str, default="ov_pose_estimation_model", help="path to model file")
    parser.add_argument("--config", type=str, default="config/ov_gpu_base.yaml", help="path to config file, different config.yaml use different config")
    parser.add_argument("--iter"  , type=int, default=600000, help="epoch num. for testing")
    parser.add_argument("--exp_id", type=int, default=0, help="")
    
    # Set default input parameter, reference demo.sh
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
    default_cad_path = os.path.join(project_root, 'SAM-6D/Data/Example/obj_000005.ply')
    default_rgb_path = os.path.join(project_root, 'SAM-6D/Data/Example/rgb.png')
    default_depth_path = os.path.join(project_root, 'SAM-6D/Data/Example/depth.png')
    default_camera_path = os.path.join(project_root, 'SAM-6D/Data/Example/camera.json')
    default_output_dir = os.path.join(project_root, 'SAM-6D/Data/Example/outputs')
    default_seg_path = os.path.join(default_output_dir, 'sam6d_results/detection_ism.json')

    parser.add_argument("--output_dir", nargs="?", default=default_output_dir, help="Path to root directory of the output")
    parser.add_argument("--cad_path", nargs="?", default=default_cad_path, help="Path to CAD(mm)")
    parser.add_argument("--rgb_path", nargs="?", default=default_rgb_path, help="Path to RGB image")
    parser.add_argument("--depth_path", nargs="?", default=default_depth_path, help="Path to Depth image(mm)")
    parser.add_argument("--cam_path", nargs="?", default=default_camera_path, help="Path to camera information")
    parser.add_argument("--seg_path", nargs="?", default=default_seg_path, help="Path to segmentation information(generated by ISM)")
    parser.add_argument("--det_score_thresh", default=0.2, help="The score threshold of detection")
    args_cfg = parser.parse_args()

    return args_cfg

# Config Init
def init():
    args = get_parser()
    exp_name = args.model + '_' + \
        osp.splitext(args.config.split("/")[-1])[0] + '_id' + str(args.exp_id)
    log_dir = osp.join("log", exp_name)

    # Load config directly from YAML file
    import yaml
    with open(args.config, 'r') as f:
        config_data = yaml.safe_load(f)
    
    # Create config object with all attributes
    class Config:
        def __init__(self, data):
            for key, value in data.items():
                if isinstance(value, dict):
                    setattr(self, key, Config(value))
                else:
                    setattr(self, key, value)
    
    cfg = Config(config_data)

    cfg.exp_name = exp_name
    cfg.device = args.device
    cfg.model_name = args.model
    cfg.log_dir = log_dir
    cfg.test_iter = args.iter

    cfg.output_dir = args.output_dir
    cfg.cad_path = args.cad_path
    cfg.rgb_path = args.rgb_path
    cfg.depth_path = args.depth_path
    cfg.cam_path = args.cam_path
    cfg.seg_path = args.seg_path

    cfg.det_score_thresh = args.det_score_thresh
    print(f"[OpenVINO] Using device: {cfg.device}")

    return cfg


def visualize(rgb, pred_rot, pred_trans, model_points, K, save_path):
    """
    Visualize the predicted pose by drawing the 3D model overlay on the RGB image.
    Args:
        rgb: np.ndarray, shape (H, W, 3), uint8, input RGB image
        pred_rot: np.ndarray, shape (N, 3, 3), predicted rotation matrices
        pred_trans: np.ndarray, shape (N, 3), predicted translations (mm)
        model_points: np.ndarray, shape (M, 3), 3D model points (mm)
        K: np.ndarray, shape (N, 3, 3), camera intrinsics
        save_path: str, path to save the visualization image
    Returns:
        concat: PIL.Image, side-by-side visualization image
    """
    img = draw_detections(rgb, pred_rot, pred_trans, model_points, K, color=(255, 0, 0))
    img = Image.fromarray(np.uint8(img))
    img.save(save_path)
    prediction = Image.open(save_path)
    
    # concat side by side in PIL
    rgb = Image.fromarray(np.uint8(rgb))
    img = np.array(img)
    concat = Image.new('RGB', (img.shape[1] + prediction.size[0], img.shape[0]))
    concat.paste(rgb, (0, 0))
    concat.paste(prediction, (img.shape[1], 0))
    return concat

def get_templates_np(path, cfg):
    """
    Load multiple rendered templates for the CAD model from disk using numpy.
    Args:
        path: str, directory containing template files
        cfg: config object, must have n_template_view, img_size, n_sample_template_point
    Returns:
        all_tem: list[np.ndarray], each (1, 3, img_size, img_size)
        all_tem_pts: list[np.ndarray], each (1, n_sample_template_point, 3)
        all_tem_choose: list[np.ndarray], each (1, n_sample_template_point)
    """
    n_template_view = cfg.n_template_view
    all_tem = []
    all_tem_choose = []
    all_tem_pts = []

    total_nView = 42
    for v in range(n_template_view):
        i = int(total_nView / n_template_view * v)
        tem, tem_choose, tem_pts = _get_template_np(path, cfg, i)
        all_tem.append(np.expand_dims(tem, axis=0))  # (1, 3, img_size, img_size)
        all_tem_choose.append(np.expand_dims(tem_choose, axis=0))  # (1, n_sample_template_point)
        all_tem_pts.append(np.expand_dims(tem_pts, axis=0))  # (1, n_sample_template_point, 3)
    return all_tem, all_tem_pts, all_tem_choose

def _get_template_np(path, cfg, tem_index=1):
    """
    Load a single template (rendered view) for the CAD model using numpy.
    Args:
        path: str, directory containing template files
        cfg: config object, must have img_size, n_sample_template_point, rgb_mask_flag
        tem_index: int, template index
    Returns:
        rgb: np.ndarray, shape (3, img_size, img_size), normalized RGB image
        rgb_choose: np.ndarray, shape (n_sample_template_point,), selected pixel indices
        xyz: np.ndarray, shape (n_sample_template_point, 3), 3D points (meters)
    """
    rgb_path = os.path.join(path, 'rgb_'+str(tem_index)+'.png')
    mask_path = os.path.join(path, 'mask_'+str(tem_index)+'.png')
    xyz_path = os.path.join(path, 'xyz_'+str(tem_index)+'.npy')

    # Load data using numpy/cv2
    rgb = cv2.imread(rgb_path, cv2.IMREAD_UNCHANGED).astype(np.uint8)
    xyz = np.load(xyz_path).astype(np.float32) / 1000.0  # Convert mm to meters
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.uint8) == 255

    # Get bounding box
    rows = np.any(mask, axis=1)
    cols = np.any(mask, axis=0)
    y1, y2 = np.where(rows)[0][[0, -1]]
    x1, x2 = np.where(cols)[0][[0, -1]]
    mask = mask[y1:y2, x1:x2]

    # Process RGB image
    rgb = rgb[:,:,::-1][y1:y2, x1:x2, :]  # BGR to RGB
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)

    # Resize RGB image
    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    
    # Normalize RGB (same as torchvision transforms)
    rgb = rgb.astype(np.float32) / 255.0
    rgb = (rgb - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
    rgb = rgb.transpose(2, 0, 1)  # (H, W, C) -> (C, H, W)

    # Process point cloud
    xyz = xyz[y1:y2, x1:x2, :].reshape((-1, 3))

    # Sample points
    choose = (mask>0).astype(np.float32).flatten().nonzero()[0]
    if len(choose) <= cfg.n_sample_template_point:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point)
    else:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=False)
    choose = choose[choose_idx]
    xyz = xyz[choose, :]

    # Calculate RGB choose indices
    h, w = y2 - y1, x2 - x1
    scale_h, scale_w = cfg.img_size / h, cfg.img_size / w
    
    choose_y = (choose // w).astype(np.float32) * scale_h
    choose_x = (choose % w).astype(np.float32) * scale_w
    rgb_choose = (choose_y * cfg.img_size + choose_x).astype(np.int32)

    return rgb, rgb_choose, xyz

def get_test_data_np(rgb_path, depth_path, cam_path, cad_path, seg_path, det_score_thresh, cfg):
    """
    Prepare test data for pose estimation using numpy.
    Args:
        rgb_path: str, path to RGB image
        depth_path: str, path to depth image
        cam_path: str, path to camera intrinsics (json)
        cad_path: str, path to CAD model
        seg_path: str, path to segmentation results (json)
        det_score_thresh: float, detection score threshold
        cfg: config object, must have n_sample_observed_point, img_size, rgb_mask_flag
    Returns:
        ret_dict: dict with keys:
            'pts': np.ndarray, (N, n_sample_observed_point, 3)
            'rgb': np.ndarray, (N, 3, img_size, img_size)
            'rgb_choose': np.ndarray, (N, n_sample_observed_point)
            'score': np.ndarray, (N,)
            'model': np.ndarray, (N, n_sample_model_point, 3)
            'K': np.ndarray, (N, 3, 3)
        whole_image: np.ndarray, (H, W, 3), original RGB image
        whole_pts: np.ndarray, (H*W, 3), full point cloud
        model_points: np.ndarray, (n_sample_model_point, 3)
        all_dets: list[dict], detection info
    """
    dets = []
    with open(seg_path) as f:
        dets_ = json.load(f) # keys: scene_id, image_id, category_id, bbox, score, segmentation
    for det in dets_:
        if det['score'] > det_score_thresh:
            dets.append(det)
    del dets_

    cam_info = json.load(open(cam_path))
    K = np.array(cam_info['cam_K']).reshape(3, 3)

    whole_image = load_im(rgb_path).astype(np.uint8)
    if len(whole_image.shape)==2:
        whole_image = np.concatenate([whole_image[:,:,None], whole_image[:,:,None], whole_image[:,:,None]], axis=2)
    whole_depth = load_im(depth_path).astype(np.float32) * cam_info['depth_scale'] / 1000.0
    whole_pts = get_point_cloud_from_depth(whole_depth, K)

    mesh = trimesh.load_mesh(cad_path)
    model_points = mesh.sample(cfg.n_sample_model_point).astype(np.float32) / 1000.0
    radius = np.max(np.linalg.norm(model_points, axis=1))

    all_rgb = []
    all_cloud = []
    all_rgb_choose = []
    all_score = []
    all_dets = []
    for inst in dets:
        seg = inst['segmentation']
        score = inst['score']

        # mask
        h,w = seg['size']
        try:
            rle = cocomask.frPyObjects(seg, h, w)
        except:
            rle = seg
        mask = cocomask.decode(rle)
        mask = np.logical_and(mask > 0, whole_depth > 0)
        if np.sum(mask) > 32:
            bbox = get_bbox(mask)
            y1, y2, x1, x2 = bbox
        else:
            continue
        mask = mask[y1:y2, x1:x2]
        choose = mask.astype(np.float32).flatten().nonzero()[0]

        # pts
        cloud = whole_pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)[choose, :]
        center = np.mean(cloud, axis=0)
        tmp_cloud = cloud - center[None, :]
        flag = np.linalg.norm(tmp_cloud, axis=1) < radius * 1.2
        if np.sum(flag) < 4:
            continue
        choose = choose[flag]
        cloud = cloud[flag]

        if len(choose) <= cfg.n_sample_observed_point:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point)
        else:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
        choose = choose[choose_idx]
        cloud = cloud[choose_idx]

        # rgb
        rgb = whole_image.copy()[y1:y2, x1:x2, :][:,:,::-1]
        if cfg.rgb_mask_flag:
            rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)
        rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
        
        # Normalize RGB (same as torchvision transforms)
        rgb = rgb.astype(np.float32) / 255.0
        rgb = (rgb - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
        rgb = rgb.transpose(2, 0, 1)  # (H, W, C) -> (C, H, W)
        
        rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)

        all_rgb.append(rgb.astype(np.float32))
        all_cloud.append(cloud.astype(np.float32))
        all_rgb_choose.append(rgb_choose.astype(np.int32))
        all_score.append(score)
        all_dets.append(inst)

    ret_dict = {}
    ret_dict['pts'] = np.stack(all_cloud)
    ret_dict['rgb'] = np.stack(all_rgb)
    ret_dict['rgb_choose'] = np.stack(all_rgb_choose)
    ret_dict['score'] = np.array(all_score, dtype=np.float32)

    ninstance = ret_dict['pts'].shape[0]
    ret_dict['model'] = np.repeat(model_points[np.newaxis, :, :], ninstance, axis=0)
    ret_dict['K'] = np.repeat(K[np.newaxis, :, :], ninstance, axis=0)
    return ret_dict, whole_image, whole_pts.reshape(-1, 3), model_points, all_dets


rgb_transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                    std=[0.229, 0.224, 0.225])])


def _get_template(path, cfg, device, tem_index=1):
    """
    Load a single template (rendered view) for the CAD model.
    Args:
        path: str, directory containing template files
        cfg: config object, must have img_size, n_sample_template_point, rgb_mask_flag
        device: str, device to place tensors on
        tem_index: int, template index
    Returns:
        rgb: torch.Tensor, shape (3, img_size, img_size), normalized RGB image
        rgb_choose: torch.Tensor, shape (n_sample_template_point,), selected pixel indices
        xyz: np.ndarray, shape (n_sample_template_point, 3), 3D points (meters)
    """
    rgb_path = os.path.join(path, 'rgb_'+str(tem_index)+'.png')
    mask_path = os.path.join(path, 'mask_'+str(tem_index)+'.png')
    xyz_path = os.path.join(path, 'xyz_'+str(tem_index)+'.npy')

    rgb = load_im(rgb_path).astype(np.uint8)
    xyz = np.load(xyz_path).astype(np.float32) / 1000.0  
    mask = load_im(mask_path).astype(np.uint8) == 255

    bbox = get_bbox(mask)
    y1, y2, x1, x2 = bbox
    mask = mask[y1:y2, x1:x2]

    rgb = rgb[:,:,::-1][y1:y2, x1:x2, :]
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)

    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    rgb = rgb_transform(np.array(rgb))

    choose = (mask>0).astype(np.float32).flatten().nonzero()[0]
    if len(choose) <= cfg.n_sample_template_point:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point)
    else:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=False)
    choose = choose[choose_idx]
    xyz = xyz[y1:y2, x1:x2, :].reshape((-1, 3))[choose, :]

    rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)
    return rgb, rgb_choose, xyz


def get_templates(path, cfg, device):
    """
    Load multiple rendered templates for the CAD model from disk.
    Args:
        path: str, directory containing template files
        cfg: config object, must have n_template_view, img_size, n_sample_template_point
        device: str, device to place tensors on
    Returns:
        all_tem: list[torch.Tensor], each (1, 3, img_size, img_size)
        all_tem_pts: list[torch.Tensor], each (1, n_sample_template_point, 3)
        all_tem_choose: list[torch.Tensor], each (1, n_sample_template_point)
    """
    n_template_view = cfg.n_template_view
    all_tem = []
    all_tem_choose = []
    all_tem_pts = []

    total_nView = 42
    for v in range(n_template_view):
        i = int(total_nView / n_template_view * v)
        tem, tem_choose, tem_pts = _get_template(path, cfg, device, i)
        all_tem.append(torch.FloatTensor(tem).unsqueeze(0).to(device))
        all_tem_choose.append(torch.IntTensor(tem_choose).long().unsqueeze(0).to(device))
        all_tem_pts.append(torch.FloatTensor(tem_pts).unsqueeze(0).to(device))
        # print("[all_tem] shape", tem.shape)
        # print("[all_tem_pts] shape", tem_pts.shape)
        # print("[all_tem_choose] shape", tem_choose.shape)
    return all_tem, all_tem_pts, all_tem_choose


def get_test_data(rgb_path, depth_path, cam_path, cad_path, seg_path, det_score_thresh, cfg, device):
    """
    Prepare test data for pose estimation.
    Args:
        rgb_path: str, path to RGB image
        depth_path: str, path to depth image
        cam_path: str, path to camera intrinsics (json)
        cad_path: str, path to CAD model
        seg_path: str, path to segmentation results (json)
        det_score_thresh: float, detection score threshold
        cfg: config object, must have n_sample_observed_point, img_size, rgb_mask_flag
        device: str, device to place tensors on
    Returns:
        input_data: dict with keys:
            'pts': torch.Tensor, (N, n_sample_observed_point, 3)
            'rgb': torch.Tensor, (N, 3, img_size, img_size)
            'rgb_choose': torch.Tensor, (N, n_sample_observed_point)
            'score': torch.Tensor, (N,)
            'model': torch.Tensor, (N, n_sample_model_point, 3)
            'K': torch.Tensor, (N, 3, 3)
        img: np.ndarray, (H, W, 3), original RGB image
        whole_pts: np.ndarray, (H*W, 3), full point cloud
        model_points: np.ndarray, (n_sample_model_point, 3)
        detections: list[dict], detection info
    """
    dets = []
    with open(seg_path) as f:
        dets_ = json.load(f) # keys: scene_id, image_id, category_id, bbox, score, segmentation
    for det in dets_:
        if det['score'] > det_score_thresh:
            dets.append(det)
    del dets_

    cam_info = json.load(open(cam_path))
    K = np.array(cam_info['cam_K']).reshape(3, 3)

    img = load_im(rgb_path).astype(np.uint8)
    if len(img.shape)==2:
        img = np.concatenate([img[:,:,None], img[:,:,None], img[:,:,None]], axis=2)
    whole_depth = load_im(depth_path).astype(np.float32) * cam_info['depth_scale'] / 1000.0
    whole_pts = get_point_cloud_from_depth(whole_depth, K)

    mesh = trimesh.load_mesh(cad_path)
    model_points = mesh.sample(cfg.n_sample_model_point).astype(np.float32) / 1000.0
    radius = np.max(np.linalg.norm(model_points, axis=1))

    all_rgb = []
    all_cloud = []
    all_rgb_choose = []
    all_score = []
    detections = []
    for inst in dets:
        seg = inst['segmentation']
        score = inst['score']

        # mask
        h,w = seg['size']
        try:
            rle = cocomask.frPyObjects(seg, h, w)
        except:
            rle = seg
        mask = cocomask.decode(rle)
        mask = np.logical_and(mask > 0, whole_depth > 0)
        if np.sum(mask) > 32:
            bbox = get_bbox(mask)
            y1, y2, x1, x2 = bbox
        else:
            continue
        mask = mask[y1:y2, x1:x2]
        choose = mask.astype(np.float32).flatten().nonzero()[0]

        # pts
        cloud = whole_pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)[choose, :]
        center = np.mean(cloud, axis=0)
        tmp_cloud = cloud - center[None, :]
        flag = np.linalg.norm(tmp_cloud, axis=1) < radius * 1.2
        if np.sum(flag) < 4:
            continue
        choose = choose[flag]
        cloud = cloud[flag]

        if len(choose) <= cfg.n_sample_observed_point:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point)
        else:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
        choose = choose[choose_idx]
        cloud = cloud[choose_idx]

        # rgb
        rgb = img.copy()[y1:y2, x1:x2, :][:,:,::-1]
        if cfg.rgb_mask_flag:
            rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)
        rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
        rgb = rgb_transform(np.array(rgb))
        rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)

        all_rgb.append(torch.FloatTensor(rgb))
        all_cloud.append(torch.FloatTensor(cloud))
        all_rgb_choose.append(torch.IntTensor(rgb_choose).long())
        all_score.append(score)
        detections.append(inst)

    input_data = {}
    input_data['pts'] = torch.stack(all_cloud).to(device)
    input_data['rgb'] = torch.stack(all_rgb).to(device)
    input_data['rgb_choose'] = torch.stack(all_rgb_choose).to(device)
    input_data['score'] = torch.FloatTensor(all_score).to(device)

    ninstance = input_data['pts'].size(0)
    input_data['model'] = torch.FloatTensor(model_points).unsqueeze(0).repeat(ninstance, 1, 1).to(device)
    input_data['K'] = torch.FloatTensor(K).unsqueeze(0).repeat(ninstance, 1, 1).to(device)
    return input_data, img, whole_pts.reshape(-1, 3), model_points, detections

class OVPEM_Sub2(nn.Module):
    def __init__(self, cfg, npoint=2048):
        super(OVPEM_Sub2, self).__init__()
        self.cfg = cfg

    def forward(self, coarse_Rt_atten, sparse_pm, sparse_po, coarse_Rt_model_pts):
        init_R, init_t = compute_coarse_Rt(coarse_Rt_atten, sparse_pm, 
                                           sparse_po, coarse_Rt_model_pts)

        return init_R, init_t

class OVPEM_Sub4(nn.Module):
    def __init__(self, cfg, npoint=2048):
        super(OVPEM_Sub4, self).__init__()
        self.cfg = cfg

    def forward(self, fine_Rt_atten, dense_pm, dense_po_out, fine_Rt_model_pts):
        pred_R, pred_t, pred_pose_score = compute_fine_Rt(fine_Rt_atten, dense_pm, 
                                           dense_po_out, fine_Rt_model_pts)

        return pred_R, pred_t, pred_pose_score

import importlib

def torch_model_init(cfg, device):
    print("=> creating model ...")
    MODEL = importlib.import_module(cfg.model_name)
    torch_model = MODEL.Net(cfg.model)
    torch_model = torch_model.to(device)
    torch_model.eval()
    checkpoint = os.path.join(os.path.dirname((os.path.abspath(__file__))), 'checkpoints', 'sam-6d-pem-base.pth')
    # load checkpoint with map_location
    if gorilla_module is not None:
        gorilla_module.solver.load_checkpoint(model=torch_model, filename=checkpoint, map_location=device)
    else:
        # Fallback: load checkpoint manually using PyTorch (partial/non-strict)
        print("Loading checkpoint using PyTorch fallback method...")
        try:
            checkpoint_data = torch.load(checkpoint, map_location=device)
            state = checkpoint_data.get('state_dict', checkpoint_data)
            if isinstance(state, dict) and 'model' in state and isinstance(state['model'], dict):
                state = state['model']
            # strip common prefixes
            def _strip_prefix(k, prefix):
                return k[len(prefix):] if k.startswith(prefix) else k
            normalized_state = {}
            for k, v in state.items():
                nk = k
                for prefix in ('module.', 'model.', 'net.'):
                    nk = _strip_prefix(nk, prefix)
                normalized_state[nk] = v
            model_state = torch_model.state_dict()
            # filter by matching keys and shapes
            filtered_state = {k: v for k, v in normalized_state.items() if k in model_state and v.shape == model_state[k].shape}
            missing_keys = [k for k in model_state.keys() if k not in normalized_state]
            unexpected_keys = [k for k in normalized_state.keys() if k not in model_state]
            print(f"Partial checkpoint load -> matched: {len(filtered_state)}/{len(model_state)}, missing: {len(missing_keys)}, unexpected: {len(unexpected_keys)}")
            model_state.update(filtered_state)
            torch_model.load_state_dict(model_state, strict=False)
            print("Checkpoint loaded with partial matching")
        except Exception as e:
            print(f"Warning: Could not load checkpoint: {e}")
            print("Continuing with uninitialized model weights")
    return torch_model

def torch_infer(cfg, torch_model, device):
    print("[PyTorch] extracting templates ...")
    tem_path = os.path.join(cfg.output_dir, 'templates')
    all_tem, all_tem_pts, all_tem_choose = get_templates(tem_path, cfg.test_dataset, device)
    
    torch_flag = True
    if torch_flag:
        # pytorch feature extraction inference
        time_start = time.time()
        with torch.no_grad():
            all_tem_pts, all_tem_feat = torch_model.feature_extraction.get_obj_feats(all_tem, all_tem_pts, all_tem_choose)
        fe_time = time.time() - time_start
        print(f"[PyTorch] fe (feature extraction) inference time: {fe_time*1000:.2f} ms")
    else:
        core = Core()
        ov_fe_model_path = "model_save/ov_fe_model.xml"
        ov_fe_model = core.read_model(ov_fe_model_path)
        ov_fe_compiled_model = core.compile_model(ov_fe_model, "GPU")
        tem_rgb_list, tem_pts_list, tem_choose_list = get_templates(tem_path, cfg.test_dataset, device)
        torch_fe_input = (tem_rgb_list, tem_pts_list, tem_choose_list)

        rgb_input = torch.cat(tem_rgb_list, dim=1)            # (B, T*3, H, W)
        pts_input = torch.cat(tem_pts_list, dim=1)            # (B, T*N, 3)
        choose_input = torch.cat(tem_choose_list, dim=1)      # (B, T*N)
        ov_fe_input = {
            "rgb_input": rgb_input,
            "pts_input": pts_input,
            "choose_input": choose_input,
        }
        time_start = time.time()
        ov_fe_results = ov_fe_compiled_model(ov_fe_input)
        ov_fe_results_list = list(ov_fe_results.values())
        fe_time = time.time() - time_start
        print(f"[OpenVINO {device}] fe (feature extraction) inference time: {fe_time*1000:.2f} ms")
        all_tem_pts = ov_fe_results_list[0]  # tem_pts_out
        all_tem_feat = ov_fe_results_list[1]  # tem_feat

        all_tem_pts = torch.from_numpy(all_tem_pts)
        all_tem_feat= torch.from_numpy(all_tem_feat)
    
    # pytorch load input data
    print("[PyTorch] loading input data ...")
    input_data, img, whole_pts, model_points, detections = get_test_data(
        cfg.rgb_path, cfg.depth_path, cfg.cam_path, cfg.cad_path, cfg.seg_path, 
        cfg.det_score_thresh, cfg.test_dataset, device
    )
    ninstance = input_data['pts'].size(0)
    
    # pytorch pose estimation inference
    print("[PyTorch] running model ...")
    pem_time_start = time.time()
    with torch.no_grad():
        input_data['dense_po'] = all_tem_pts.repeat(ninstance,1,1)
        input_data['dense_fo'] = all_tem_feat.repeat(ninstance,1,1)
        model_input_tuple = (
            input_data['pts'], input_data['rgb'], input_data['rgb_choose'], 
            input_data['model'], input_data['dense_po'], input_data['dense_fo']
        )
        pred_R, pred_t, pred_pose_score = torch_model(*model_input_tuple)
        pred_R = pred_R.detach().cpu().numpy()
        pred_t = pred_t.detach().cpu().numpy()
        pred_pose_score = pred_pose_score.detach().cpu().numpy()
        # out = model(input_data)
    pem_time = time.time() - pem_time_start
    print(f"[PyTorch] pose estimation inference time: {pem_time*1000:.2f} ms")

    # pytorch save results
    if 'pred_pose_score' in input_data.keys():
        pose_scores = pred_pose_score * input_data['score']
    else:
        pose_scores = input_data['score']
    pose_scores = pose_scores.detach().cpu().numpy()
    pred_rot = pred_R
    pred_trans = pred_t * 1000
    print("[PyTorch] visualizating ...")
    os.makedirs(f"{cfg.output_dir}/sam6d_results", exist_ok=True)
    for idx, det in enumerate(detections):
        detections[idx]['score'] = float(pose_scores[idx])
        detections[idx]['R'] = list(pred_rot[idx].tolist())
        detections[idx]['t'] = list(pred_trans[idx].tolist())

    with open(os.path.join(f"{cfg.output_dir}/sam6d_results", f'detection_pem_{cfg.device}.json'), "w") as f:
        json.dump(detections, f)

    print("[PyTorch] visualizating ...")
    save_path = os.path.join(f"{cfg.output_dir}/sam6d_results", f'vis_pem_{cfg.device}.png')
    valid_masks = pose_scores == pose_scores.max()
    K = input_data['K'].detach().cpu().numpy()[valid_masks]
    vis_img = visualize(img, pred_rot[valid_masks], pred_trans[valid_masks], model_points*1000, K, save_path)
    vis_img.save(save_path)
    print(f"[Torch Inference Done] Pose_Estimation_Model ({cfg.device} Version)") 
    print(f"[PyTorch] PEM E2E Inference Time: {(fe_time + pem_time):.2f} s")

def ov_model_init(core, model_dir="model_save", device="CPU"):
    ov_fe_model_path = os.path.join(model_dir, "ov_fe_model.xml")
    ov_pem_sub1_model_path = os.path.join(model_dir, "ov_pem_sub1_model.xml")
    ov_pem_sub2_model_path = os.path.join(model_dir, "ov_pem_sub2_model.xml")
    ov_pem_sub3_model_path = os.path.join(model_dir, "ov_pem_sub3_model.xml")
    ov_pem_sub4_model_path = os.path.join(model_dir, "ov_pem_sub4_model.xml")

    ov_gpu_kernel_path = "./model/ov_pointnet2_op/ov_gpu_custom_op.xml"
    ov_extension_lib_path = "./model/ov_pointnet2_op/build/libopenvino_operation_extension.so"
    core.add_extension(ov_extension_lib_path)

    if device == "GPU":
        core.set_property("GPU", {"INFERENCE_PRECISION_HINT": "f32"})
        core.set_property("GPU", {"CONFIG_FILE": ov_gpu_kernel_path})

    # ov load models
    ov_fe_model = core.read_model(ov_fe_model_path)
    ov_pem_sub1_model = core.read_model(ov_pem_sub1_model_path)
    ov_pem_sub2_model = core.read_model(ov_pem_sub2_model_path)
    ov_pem_sub3_model = core.read_model(ov_pem_sub3_model_path)
    ov_pem_sub4_model = core.read_model(ov_pem_sub4_model_path)

    ov_fe_compiled_model = core.compile_model(ov_fe_model, device)
    ov_pem_sub1_model_compiled = core.compile_model(ov_pem_sub1_model, device)
    ov_pem_sub2_model_compiled = core.compile_model(ov_pem_sub2_model, "CPU")
    ov_pem_sub3_model_compiled = core.compile_model(ov_pem_sub3_model, device)
    ov_pem_sub4_model_compiled = core.compile_model(ov_pem_sub4_model, "CPU")

    ov_model_list = [ov_fe_compiled_model, 
                    ov_pem_sub1_model_compiled, ov_pem_sub2_model_compiled,
                    ov_pem_sub3_model_compiled, ov_pem_sub4_model_compiled,
                    ]
    return ov_model_list

def ov_infer(cfg, ov_model_list, input_data, device, torch_model, torch_device):
    print(f"[OpenVINO {device}] OV pem pipeline inference start...")
    # ================OpenVINO Inference==========================

    tem_path = os.path.join(cfg.output_dir, 'templates')
    all_tem, all_tem_pts, all_tem_choose = get_templates(tem_path, cfg.test_dataset, torch_device)
    
    rgb_input = torch.cat(all_tem, dim=1)            # (B, T*3, H, W)
    pts_input = torch.cat(all_tem_pts, dim=1)            # (B, T*N, 3)
    choose_input = torch.cat(all_tem_choose, dim=1)      # (B, T*N)
    ov_fe_input = {
        "rgb_input": rgb_input,
        "pts_input": pts_input,
        "choose_input": choose_input,
    }

    torch_flag = False
    if torch_flag:
        time_start = time.time()
        with torch.no_grad():
            all_tem_pts, all_tem_feat = torch_model.feature_extraction.get_obj_feats(all_tem, all_tem_pts, all_tem_choose)
        fe_time = time.time() - time_start
        print(f"[PyTorch] fe (feature extraction) inference time: {fe_time*1000:.2f} ms")
    else:
        ov_fe_compiled_model = ov_model_list[0]
        time_start = time.time()
        ov_fe_results = ov_fe_compiled_model(ov_fe_input)
        ov_fe_results_list = list(ov_fe_results.values())
        fe_time = time.time() - time_start
        print(f"[OpenVINO {device}] fe (feature extraction) inference time: {fe_time*1000:.2f} ms")
        all_tem_pts = ov_fe_results_list[0]  # tem_pts_out
        all_tem_feat = ov_fe_results_list[1]  # tem_feat

    # ================[Pass] OpenVINO FE Done==========================
    ninstance = input_data['pts'].shape[0]
    dense_po = np.repeat(all_tem_pts, ninstance, axis=0)
    dense_fo = np.repeat(all_tem_feat, ninstance, axis=0)

    pts = input_data['pts']
    rgb = input_data['rgb']
    rgb_choose = input_data['rgb_choose']
    model = input_data['model']

    ov_pem_sub1_model_compiled = ov_model_list[1]
    ov_pem_sub1_inputs = {
            "pts": pts,
            "rgb": rgb,
            "rgb_choose": rgb_choose,
            "model": model,
            "dense_po": dense_po,
            "dense_fo": dense_fo,
        }
    time_start = time.time()
    ov_pem_sub1_results = ov_pem_sub1_model_compiled(ov_pem_sub1_inputs)
    ov_pem_sub1_results = list(ov_pem_sub1_results.values())
    pem_time = time.time() - time_start
    print(f"[OpenVINO {device}] ov_pem_sub1 inference time: {pem_time*1000:.2f} ms")

    coarse_Rt_atten = ov_pem_sub1_results[0]
    sparse_pm = ov_pem_sub1_results[1]
    sparse_po = ov_pem_sub1_results[2]
    coarse_Rt_model_pts = ov_pem_sub1_results[3]
    dense_pm = ov_pem_sub1_results[4]
    dense_fm = ov_pem_sub1_results[5]
    geo_embedding_m = ov_pem_sub1_results[6]
    fps_idx_m = ov_pem_sub1_results[7]
    dense_po_out = ov_pem_sub1_results[8] 
    dense_fo_out = ov_pem_sub1_results[9]
    geo_embedding_o = ov_pem_sub1_results[10]
    fps_idx_o = ov_pem_sub1_results[11]
    radius = ov_pem_sub1_results[12]

    # ===============[Pass]OpenVINO Sub1 model ===========================

    torch_pem_sub2_input =  (torch.from_numpy(coarse_Rt_atten), torch.from_numpy(sparse_pm), 
                             torch.from_numpy(sparse_po), torch.from_numpy(coarse_Rt_model_pts))
    torch_flag = True
    if torch_flag:
        print("[IPEX Infer] pem_sub2 Start")
        pem_sub2_model = OVPEM_Sub2(cfg.model)
        torch_device = torch.device("xpu")
        pem_sub2_model.to(torch_device)
        torch_pem_input_new = []
        for tmp_tensor in torch_pem_sub2_input:
            torch_pem_input_new.append(tmp_tensor.to(torch_device))
        time_start = time.time()
        with torch.no_grad():
            init_R, init_t = pem_sub2_model(*torch_pem_input_new)
            init_R = init_R.cpu().numpy()
            init_t = init_t.cpu().numpy()
        torch.xpu.empty_cache()
        fe_time = time.time() - time_start
        print(f"[IPEX Infer] pem_sub2 inference time: {fe_time*1000:.2f} ms")

        # print("[Pytorch Infer] pem_sub2 Start")
        # pem_sub2_model = OVPEM_Sub2(cfg.model)
        # torch_device = torch.device("cpu")
        # pem_sub2_model.to(torch_device)
        # torch_pem_input_new = []
        # for tmp_tensor in torch_pem_sub2_input:
        #     torch_pem_input_new.append(tmp_tensor.to(torch_device))
        # time_start = time.time()
        # with torch.no_grad():
        #     init_R, init_t = pem_sub2_model(*torch_pem_input_new)
        #     init_R = init_R.cpu().numpy()
        #     init_t = init_t.cpu().numpy()
        # torch.xpu.empty_cache()
        # fe_time = time.time() - time_start
        # print(f"[Pytorch Infer] pem_sub2 inference time: {fe_time*1000:.2f} ms")

    else:
        ov_pem_sub2_input = {
            "coarse_Rt_atten": coarse_Rt_atten,
            "sparse_pm": sparse_pm,
            "sparse_po": sparse_po,
            "coarse_Rt_model_pts": coarse_Rt_model_pts
        }
        ov_pem_sub2_model_compiled = ov_model_list[2]
        time_start = time.time()
        ov_pem_sub2_results = ov_pem_sub2_model_compiled(ov_pem_sub2_input)
        ov_pem_sub2_results = list(ov_pem_sub2_results.values())
        pem_time = time.time() - time_start
        print(f"[OpenVINO {device}] ov_pem_sub2 inference time: {pem_time*1000:.2f} ms")
        
        init_R = ov_pem_sub2_results[0]
        init_t = ov_pem_sub2_results[1]
    # ================[Pass]OpenVINO Sub2 model ==========================

    ov_pem_sub3_model_compiled = ov_model_list[3]
    ov_pem_sub3_input = {
                "dense_pm": dense_pm, 
                "dense_fm": dense_fm, 
                "geo_embedding_m": geo_embedding_m, 
                "fps_idx_m": fps_idx_m,
                "dense_po_out": dense_po_out, 
                "dense_fo_out": dense_fo_out, 
                "geo_embedding_o": geo_embedding_o, 
                "fps_idx_o": fps_idx_o,
                "radius": radius, 
                "model": model, 
                "init_R": init_R, 
                "init_t": init_t}
    time_start = time.time()
    ov_pem_sub3_results = ov_pem_sub3_model_compiled(ov_pem_sub3_input)
    ov_pem_sub3_results = list(ov_pem_sub3_results.values())
    pem_time = time.time() - time_start
    print(f"[OpenVINO {device}] ov_pem_sub3 inference time: {pem_time*1000:.2f} ms")

    fine_Rt_atten = ov_pem_sub3_results[0]
    fine_Rt_model_pts = ov_pem_sub3_results[1]
    # ==============[Pass]OpenVINO Sub3 model ============================
    torch_pem_sub4_input = (torch.from_numpy(fine_Rt_atten), torch.from_numpy(dense_pm), \
                            torch.from_numpy(dense_po_out), torch.from_numpy(fine_Rt_model_pts))
    torch_flag = False
    if torch_flag:
        print("[Pytorch Infer] pem_sub4 Start")
        pem_sub4_model = OVPEM_Sub4(cfg.model)
        torch_device = torch.device("cpu")
        pem_sub4_model.to(torch_device)
        torch_pem_input_new = []
        for tmp_tensor in torch_pem_sub4_input:
            torch_pem_input_new.append(tmp_tensor.to(torch_device))
        time_start = time.time()
        with torch.no_grad():
            pred_R, pred_t, pred_pose_score = pem_sub4_model(*torch_pem_input_new)
            pred_R = pred_R.cpu().numpy()
            pred_t = pred_t.cpu().numpy()
            pred_pose_score = pred_pose_score.cpu().numpy()
        torch.xpu.empty_cache()
        fe_time = time.time() - time_start
        print(f"[Pytorch Infer] pem_sub4 inference time: {fe_time*1000:.2f} ms")
    else:
        ov_pem_sub4_model_compiled = ov_model_list[4]
        ov_pem_sub4_input = {
            "fine_Rt_atten": fine_Rt_atten,
            "dense_pm": dense_pm,
            "dense_po_out": dense_po_out,
            "fine_Rt_model_pts": fine_Rt_model_pts
        }
        time_start = time.time()
        ov_pem_sub4_results = ov_pem_sub4_model_compiled(ov_pem_sub4_input)
        ov_pem_sub4_results = list(ov_pem_sub4_results.values())
        pem_time = time.time() - time_start
        print(f"[OpenVINO {device}] ov_pem_sub4 inference time: {pem_time*1000:.2f} ms")
        
        pred_R = ov_pem_sub4_results[0]
        pred_t = ov_pem_sub4_results[1]
        pred_pose_score = ov_pem_sub4_results[2]
    # ==============[Pass]OpenVINO Sub4 model ============================
    return pred_R, pred_t, pred_pose_score


def main():
    # init config
    cfg = init()
    seed = cfg.rd_seed

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    # ov init
    core = Core()
    device = cfg.device

    torch_device = torch.device("cpu")
    torch_model = torch_model_init(cfg, torch_device)
    # torch_infer(cfg, torch_model, torch_device)

    #======================================

    model_dir="model_save" 

    input_data, img, whole_pts, model_points, detections = get_test_data(
        cfg.rgb_path, cfg.depth_path, cfg.cam_path, cfg.cad_path, cfg.seg_path, 
        cfg.det_score_thresh, cfg.test_dataset, torch_device)

    ov_model_list = ov_model_init(core, model_dir, device)
    
    # Warmup
    ov_pred_R, ov_pred_t, ov_pred_pose_score = ov_infer(cfg, ov_model_list,input_data, device, torch_model, torch_device)
    print("=================== [Warmup] ====================")
    ov_infer_start = time.time()
    ov_pred_R, ov_pred_t, ov_pred_pose_score = ov_infer(cfg, ov_model_list,input_data, device, torch_model, torch_device)
    ov_infer_end = time.time()

    pose_scores = ov_pred_pose_score * input_data['score'].detach().cpu().numpy()
    pred_rot = ov_pred_R
    pred_trans = ov_pred_t * 100

    # Save results
    print("[OpenVINO] saving results ...")
    os.makedirs(f"{cfg.output_dir}/sam6d_results", exist_ok=True)
    for idx, det in enumerate(detections):
        detections[idx]['score'] = float(pose_scores[idx])
        detections[idx]['R'] = list(pred_rot[idx].tolist())
        detections[idx]['t'] = list(pred_trans[idx].tolist())

    with open(os.path.join(f"{cfg.output_dir}/sam6d_results", f'detection_pem_ov_{device}.json'), "w") as f:
        json.dump(detections, f)

    print("[OpenVINO] visualizating ...")
    save_path = os.path.join(f"{cfg.output_dir}/sam6d_results", f'vis_pem_ov_{device}.png')
    valid_masks = pose_scores == pose_scores.max()
    # K = input_data['K'][valid_masks]
    K = input_data['K'].detach().cpu().numpy()[valid_masks]
    vis_img = visualize(img, pred_rot[valid_masks], pred_trans[valid_masks], model_points*1000, K, save_path)
    vis_img.save(save_path)
    print(f"[OpenVINO Inference Done] Pose_Estimation_Model ({device} Version)") 
    print(f"[OpenVINO] PEM E2E Inference Time: {(ov_infer_end - ov_infer_start):.2f} s, \n save_path : {save_path}")    

if __name__ == "__main__":
    main()