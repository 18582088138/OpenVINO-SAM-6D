import gorilla
import argparse
import os
import sys
from PIL import Image
import os.path as osp
import numpy as np
import random
import importlib
import json
import cv2

import torch
import torch.nn as nn
import torchvision.transforms as transforms

import openvino as ov
from openvino import Core

import pycocotools.mask as cocomask
import trimesh

from utils.data_utils import load_im, get_bbox, get_point_cloud_from_depth, get_resize_rgb_choose
from utils.draw_utils import draw_detections

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.join(BASE_DIR, '..', 'Pose_Estimation_Model')
sys.path.append(os.path.join(ROOT_DIR, 'provider'))
sys.path.append(os.path.join(ROOT_DIR, 'utils'))
sys.path.append(os.path.join(ROOT_DIR, 'model'))
sys.path.append(os.path.join(BASE_DIR, 'model', 'pointnet2'))

def get_parser():
    parser = argparse.ArgumentParser(description="Pose Estimation Model Convert to ONNX and OpenVINO (CPU Version)")
    # pem
    parser.add_argument("--device", type=str, default="cpu", help="device to run on (cpu/cuda)")
    parser.add_argument("--model" , type=str, default="pose_estimation_model", help="path to model file")
    parser.add_argument("--config", type=str, default="config/base.yaml", help="path to config file, different config.yaml use different config")
    parser.add_argument("--iter"  , type=int, default=600000, help="epoch num. for testing")
    parser.add_argument("--exp_id", type=int, default=0, help="")
    
    # Set default input parameter, reference demo.sh
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
    default_cad_path = os.path.join(project_root, 'SAM-6D/Data/Example/obj_000005.ply')
    default_rgb_path = os.path.join(project_root, 'SAM-6D/Data/Example/rgb.png')
    default_depth_path = os.path.join(project_root, 'SAM-6D/Data/Example/depth.png')
    default_camera_path = os.path.join(project_root, 'SAM-6D/Data/Example/camera.json')
    default_output_dir = os.path.join(project_root, 'SAM-6D/Data/Example/outputs')
    default_seg_path = os.path.join(default_output_dir, 'sam6d_results/detection_ism.json')

    parser.add_argument("--output_dir", nargs="?", default=default_output_dir, help="Path to root directory of the output")
    parser.add_argument("--cad_path", nargs="?", default=default_cad_path, help="Path to CAD(mm)")
    parser.add_argument("--rgb_path", nargs="?", default=default_rgb_path, help="Path to RGB image")
    parser.add_argument("--depth_path", nargs="?", default=default_depth_path, help="Path to Depth image(mm)")
    parser.add_argument("--cam_path", nargs="?", default=default_camera_path, help="Path to camera information")
    parser.add_argument("--seg_path", nargs="?", default=default_seg_path, help="Path to segmentation information(generated by ISM)")
    parser.add_argument("--det_score_thresh", default=0.2, help="The score threshold of detection")
    args_cfg = parser.parse_args()

    return args_cfg

# Config Init
def init():
    args = get_parser()
    exp_name = args.model + '_' + \
        osp.splitext(args.config.split("/")[-1])[0] + '_id' + str(args.exp_id)
    log_dir = osp.join("log", exp_name)

    cfg = gorilla.Config.fromfile(args.config)
    cfg.exp_name = exp_name
    cfg.device = args.device
    cfg.model_name = args.model
    cfg.log_dir = log_dir
    cfg.test_iter = args.iter

    cfg.output_dir = args.output_dir
    cfg.cad_path = args.cad_path
    cfg.rgb_path = args.rgb_path
    cfg.depth_path = args.depth_path
    cfg.cam_path = args.cam_path
    cfg.seg_path = args.seg_path

    cfg.det_score_thresh = args.det_score_thresh
    
    if cfg.device == "cuda" and not torch.cuda.is_available():
        print("CUDA is not available, automatically switch to CPU")
        cfg.device = "cpu"
    
    print(f"Using device: {cfg.device}")

    return cfg

rgb_transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                    std=[0.229, 0.224, 0.225])])

def visualize(rgb, pred_rot, pred_trans, model_points, K, save_path):
    """
    Visualize the predicted pose by drawing the 3D model overlay on the RGB image.
    Args:
        rgb: np.ndarray, shape (H, W, 3), uint8, input RGB image
        pred_rot: np.ndarray, shape (N, 3, 3), predicted rotation matrices
        pred_trans: np.ndarray, shape (N, 3), predicted translations (mm)
        model_points: np.ndarray, shape (M, 3), 3D model points (mm)
        K: np.ndarray, shape (N, 3, 3), camera intrinsics
        save_path: str, path to save the visualization image
    Returns:
        concat: PIL.Image, side-by-side visualization image
    """
    img = draw_detections(rgb, pred_rot, pred_trans, model_points, K, color=(255, 0, 0))
    img = Image.fromarray(np.uint8(img))
    img.save(save_path)
    prediction = Image.open(save_path)
    
    # concat side by side in PIL
    rgb = Image.fromarray(np.uint8(rgb))
    img = np.array(img)
    concat = Image.new('RGB', (img.shape[1] + prediction.size[0], img.shape[0]))
    concat.paste(rgb, (0, 0))
    concat.paste(prediction, (img.shape[1], 0))
    return concat


def _get_template(path, cfg, device, tem_index=1):
    """
    Load a single template (rendered view) for the CAD model.
    Args:
        path: str, directory containing template files
        cfg: config object, must have img_size, n_sample_template_point, rgb_mask_flag
        device: str, device to place tensors on
        tem_index: int, template index
    Returns:
        rgb: torch.Tensor, shape (3, img_size, img_size), normalized RGB image
        rgb_choose: torch.Tensor, shape (n_sample_template_point,), selected pixel indices
        xyz: np.ndarray, shape (n_sample_template_point, 3), 3D points (meters)
    """
    rgb_path = os.path.join(path, 'rgb_'+str(tem_index)+'.png')
    mask_path = os.path.join(path, 'mask_'+str(tem_index)+'.png')
    xyz_path = os.path.join(path, 'xyz_'+str(tem_index)+'.npy')

    rgb = load_im(rgb_path).astype(np.uint8)
    xyz = np.load(xyz_path).astype(np.float32) / 1000.0  
    mask = load_im(mask_path).astype(np.uint8) == 255

    bbox = get_bbox(mask)
    y1, y2, x1, x2 = bbox
    mask = mask[y1:y2, x1:x2]

    rgb = rgb[:,:,::-1][y1:y2, x1:x2, :]
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)

    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    rgb = rgb_transform(np.array(rgb))

    choose = (mask>0).astype(np.float32).flatten().nonzero()[0]
    if len(choose) <= cfg.n_sample_template_point:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point)
    else:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=False)
    choose = choose[choose_idx]
    xyz = xyz[y1:y2, x1:x2, :].reshape((-1, 3))[choose, :]

    rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)
    return rgb, rgb_choose, xyz


def get_templates(path, cfg, device):
    """
    Load multiple rendered templates for the CAD model from disk.
    Args:
        path: str, directory containing template files
        cfg: config object, must have n_template_view, img_size, n_sample_template_point
        device: str, device to place tensors on
    Returns:
        all_tem: list[torch.Tensor], each (1, 3, img_size, img_size)
        all_tem_pts: list[torch.Tensor], each (1, n_sample_template_point, 3)
        all_tem_choose: list[torch.Tensor], each (1, n_sample_template_point)
    """
    n_template_view = cfg.n_template_view
    all_tem = []
    all_tem_choose = []
    all_tem_pts = []

    total_nView = 42
    for v in range(n_template_view):
        i = int(total_nView / n_template_view * v)
        tem, tem_choose, tem_pts = _get_template(path, cfg, device, i)
        all_tem.append(torch.FloatTensor(tem).unsqueeze(0).to(device))
        all_tem_choose.append(torch.IntTensor(tem_choose).long().unsqueeze(0).to(device))
        all_tem_pts.append(torch.FloatTensor(tem_pts).unsqueeze(0).to(device))
    return all_tem, all_tem_pts, all_tem_choose


def get_test_data(rgb_path, depth_path, cam_path, cad_path, seg_path, det_score_thresh, cfg, device):
    """
    Prepare test data for pose estimation.
    Args:
        rgb_path: str, path to RGB image
        depth_path: str, path to depth image
        cam_path: str, path to camera intrinsics (json)
        cad_path: str, path to CAD model
        seg_path: str, path to segmentation results (json)
        det_score_thresh: float, detection score threshold
        cfg: config object, must have n_sample_observed_point, img_size, rgb_mask_flag
        device: str, device to place tensors on
    Returns:
        ret_dict: dict with keys:
            'pts': torch.Tensor, (N, n_sample_observed_point, 3)
            'rgb': torch.Tensor, (N, 3, img_size, img_size)
            'rgb_choose': torch.Tensor, (N, n_sample_observed_point)
            'score': torch.Tensor, (N,)
            'model': torch.Tensor, (N, n_sample_model_point, 3)
            'K': torch.Tensor, (N, 3, 3)
        whole_image: np.ndarray, (H, W, 3), original RGB image
        whole_pts: np.ndarray, (H*W, 3), full point cloud
        model_points: np.ndarray, (n_sample_model_point, 3)
        all_dets: list[dict], detection info
    """
    dets = []
    with open(seg_path) as f:
        dets_ = json.load(f) # keys: scene_id, image_id, category_id, bbox, score, segmentation
    for det in dets_:
        if det['score'] > det_score_thresh:
            dets.append(det)
    del dets_

    cam_info = json.load(open(cam_path))
    K = np.array(cam_info['cam_K']).reshape(3, 3)

    whole_image = load_im(rgb_path).astype(np.uint8)
    if len(whole_image.shape)==2:
        whole_image = np.concatenate([whole_image[:,:,None], whole_image[:,:,None], whole_image[:,:,None]], axis=2)
    whole_depth = load_im(depth_path).astype(np.float32) * cam_info['depth_scale'] / 1000.0
    whole_pts = get_point_cloud_from_depth(whole_depth, K)

    mesh = trimesh.load_mesh(cad_path)
    model_points = mesh.sample(cfg.n_sample_model_point).astype(np.float32) / 1000.0
    radius = np.max(np.linalg.norm(model_points, axis=1))

    all_rgb = []
    all_cloud = []
    all_rgb_choose = []
    all_score = []
    all_dets = []
    for inst in dets:
        seg = inst['segmentation']
        score = inst['score']

        # mask
        h,w = seg['size']
        try:
            rle = cocomask.frPyObjects(seg, h, w)
        except:
            rle = seg
        mask = cocomask.decode(rle)
        mask = np.logical_and(mask > 0, whole_depth > 0)
        if np.sum(mask) > 32:
            bbox = get_bbox(mask)
            y1, y2, x1, x2 = bbox
        else:
            continue
        mask = mask[y1:y2, x1:x2]
        choose = mask.astype(np.float32).flatten().nonzero()[0]

        # pts
        cloud = whole_pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)[choose, :]
        center = np.mean(cloud, axis=0)
        tmp_cloud = cloud - center[None, :]
        flag = np.linalg.norm(tmp_cloud, axis=1) < radius * 1.2
        if np.sum(flag) < 4:
            continue
        choose = choose[flag]
        cloud = cloud[flag]

        if len(choose) <= cfg.n_sample_observed_point:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point)
        else:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
        choose = choose[choose_idx]
        cloud = cloud[choose_idx]

        # rgb
        rgb = whole_image.copy()[y1:y2, x1:x2, :][:,:,::-1]
        if cfg.rgb_mask_flag:
            rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)
        rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
        rgb = rgb_transform(np.array(rgb))
        rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)

        all_rgb.append(torch.FloatTensor(rgb))
        all_cloud.append(torch.FloatTensor(cloud))
        all_rgb_choose.append(torch.IntTensor(rgb_choose).long())
        all_score.append(score)
        all_dets.append(inst)

    ret_dict = {}
    ret_dict['pts'] = torch.stack(all_cloud).to(device)
    ret_dict['rgb'] = torch.stack(all_rgb).to(device)
    ret_dict['rgb_choose'] = torch.stack(all_rgb_choose).to(device)
    ret_dict['score'] = torch.FloatTensor(all_score).to(device)

    ninstance = ret_dict['pts'].size(0)
    ret_dict['model'] = torch.FloatTensor(model_points).unsqueeze(0).repeat(ninstance, 1, 1).to(device)
    ret_dict['K'] = torch.FloatTensor(K).unsqueeze(0).repeat(ninstance, 1, 1).to(device)
    return ret_dict, whole_image, whole_pts.reshape(-1, 3), model_points, all_dets

def create_dummy_templates(cfg, device, batch_size=1):
    # create dummy templates for onnx export
    # [ToDo] If using orginial data for fe_wrapper export, failed due to : 
    # [ONNX] feature extraction submodel export failed: too many indices for tensor of dimension 2
    n_template_view = cfg.test_dataset.n_template_view
    img_size = cfg.test_dataset.img_size
    n_sample_template_point = cfg.test_dataset.n_sample_template_point
    
    tem_rgb_concat = torch.randn(batch_size, n_template_view * 3, img_size, img_size, device=device)
    tem_pts_concat = torch.randn(batch_size, n_template_view * n_sample_template_point, 3, device=device)
    tem_choose_concat = torch.randint(0, img_size*img_size, (batch_size, n_template_view * n_sample_template_point), device=device)
    
    return tem_rgb_concat, tem_pts_concat, tem_choose_concat

def prepare_data(cfg, model, device):
    print("[Pytorch] extracting templates ...")
    tem_path = os.path.join(cfg.output_dir, 'templates')
    all_tem, all_tem_pts, all_tem_choose = get_templates(tem_path, cfg.test_dataset, device)
    with torch.no_grad():
        all_tem_pts, all_tem_feat = model.feature_extraction.get_obj_feats(all_tem, all_tem_pts, all_tem_choose)

    print("[Pytorch] loading input data ...")
    input_data, img, whole_pts, model_points, detections = get_test_data(
        cfg.rgb_path, cfg.depth_path, cfg.cam_path, cfg.cad_path, cfg.seg_path, 
        cfg.det_score_thresh, cfg.test_dataset, device
    )
    # print(f"[Pytorch] input_data.keys: {input_data.keys()}")
    ninstance = input_data['pts'].size(0)

    #pytorch infer verification
    print("[Pytorch] running model ...")
    with torch.no_grad():
        input_data['dense_po'] = all_tem_pts.repeat(ninstance,1,1)
        input_data['dense_fo'] = all_tem_feat.repeat(ninstance,1,1)

    print("[Pytorch] model inference success")

    print("[Pytorch] prepare data for model convert...")
    onnx_pem_input_name = ["pts", "rgb", "rgb_choose", "model", "dense_po", "dense_fo"]
    onnx_pem_output_name = ["pred_R", "pred_t", "pred_pose_score"]
    onnx_pem_input = (input_data['pts'], input_data['rgb'], input_data['rgb_choose'], 
                      input_data['model'], input_data['dense_po'], input_data['dense_fo'])

    rgb_input, pts_input, choose_input = create_dummy_templates(cfg, device)
    onnx_fe_input_name = ["rgb_input", "pts_input", "choose_input"]
    onnx_fe_output_name = ["pts_output", "feat_output"]
    onnx_fe_input = (rgb_input, pts_input, choose_input)

    batch_size = -1
    ov_pem_input_name = {"pts":[batch_size,2048,3], 
                        "rgb":[batch_size,3,224,224], 
                        "rgb_choose":[batch_size,2048], 
                        "model":[batch_size,1024,3], 
                        "dense_po":[batch_size,2048,3], 
                        "dense_fo":[batch_size,2048,256]}
    ov_pem_output_name = {"pred_R":[batch_size,3,3], 
                          "pred_t":[batch_size,3], 
                          "pred_pose_score":[batch_size]}
    ov_pem_input = {
        "pts": input_data['pts'],
        "rgb": input_data['rgb'],
        "rgb_choose": input_data['rgb_choose'],
        "model": input_data['model'],
        "dense_po": input_data['dense_po'],
        "dense_fo": input_data['dense_fo'],
    }

    ov_fe_input_name = {"rgb_input":[batch_size,3,224,224], 
                                        "pts_input":[batch_size,2048,3], 
                                        "choose_input":[batch_size,2048]}
    ov_fe_output_name = {"pts_output":[batch_size,2048,3], 
                                        "feat_output":[batch_size,2048,256]}
    ov_fe_input = {
        "rgb_input": rgb_input,
        "pts_input": pts_input,
        "choose_input": choose_input,
    }

    return (onnx_pem_input_name, onnx_pem_output_name, onnx_pem_input,
            onnx_fe_input_name, onnx_fe_output_name, onnx_fe_input,
            ov_pem_input_name, ov_pem_output_name, ov_pem_input,
            ov_fe_input_name, ov_fe_output_name, ov_fe_input)

class FeatureExtractionWrapper(nn.Module):
    """feature_extraction get_obj_feats wrapper for onnx export"""
    def __init__(self, feature_extraction):
        super().__init__()
        self.feature_extraction = feature_extraction
    
    def forward(self, rgb_input, pts_input, choose_input):
        """
        export get_obj_feats method
        input: concatenated tensors
        rgb_input: (B, n_template_view*3, H, W)
        pts_input: (B, n_template_view*n_sample_template_point, 3)
        choose_input: (B, n_template_view*n_sample_template_point)
        """
        # debug info
        print(f"FeatureExtractionWrapper输入形状: rgb_input={rgb_input.shape}, pts_input={pts_input.shape}, choose_input={choose_input.shape}")
        
        # split concatenated tensors
        n_template_view = 42  # from config
        n_sample_template_point = pts_input.size(1) // n_template_view
        
        print(f"split parameters: n_template_view={n_template_view}, n_sample_template_point={n_sample_template_point}")
        
        # split rgb tensor
        tem_rgb_list = []
        for i in range(n_template_view):
            start_idx = i * 3
            end_idx = (i + 1) * 3
            tem_rgb_list.append(rgb_input[:, start_idx:end_idx, :, :])
        
        # split pts tensor
        tem_pts_list = []
        for i in range(n_template_view):
            start_idx = i * n_sample_template_point
            end_idx = (i + 1) * n_sample_template_point
            tem_pts_list.append(pts_input[:, start_idx:end_idx, :])
        
        # split choose tensor
        tem_choose_list = []
        for i in range(n_template_view):
            start_idx = i * n_sample_template_point
            end_idx = (i + 1) * n_sample_template_point
            tem_choose_list.append(choose_input[:, start_idx:end_idx])
        
        # call original method
        return self.feature_extraction.get_obj_feats(tem_rgb_list, tem_pts_list, tem_choose_list)

def onnx_model_convert_feature_extraction_submodel(model, onnx_fe_input_name, onnx_fe_output_name, onnx_fe_input, onnx_model_path):
    feature_wrapper = FeatureExtractionWrapper(model.feature_extraction)
    try:
        with torch.no_grad():
            torch.onnx.export(
                feature_wrapper,
                onnx_fe_input,
                onnx_model_path,
                opset_version=20,
                operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK,
                input_names=onnx_fe_input_name,
                output_names=onnx_fe_output_name,
                dynamic_axes={k: {0: "batch"} for k in onnx_fe_input_name},
                do_constant_folding=False,
                verbose=False,  # True , for detailed output
                export_params=True,
                keep_initializers_as_inputs=False
            )
        print(f"[ONNX] feature extraction submodel export success: {onnx_model_path}")
        
    except Exception as e:
        print(f"[ONNX] feature extraction submodel export failed: {e}")

def onnx_model_convert_pose_estimation_submodel(model, onnx_pem_input_name, onnx_pem_output_name, onnx_pem_input, onnx_model_path):
    try:
        torch.onnx.export(
            model,
            onnx_pem_input,
            onnx_model_path,
            opset_version=20,
            operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK,
            input_names=onnx_pem_input_name,
            output_names=onnx_pem_output_name,
            dynamic_axes={k: {0: "batch"} for k in onnx_pem_input_name},
            do_constant_folding=False,
            verbose=False,
            export_params=True,
            keep_initializers_as_inputs=False
        )
        print(f"[ONNX] PEM onnx model export success: {onnx_model_path}")
            
    except Exception as e:
        print(f"[ONNX] export failed : {e}")


def openvino_model_convert_feature_extraction_submodel(core, ov_fe_input_name, ov_fe_output_name, ov_fe_input, onnx_fe_model_path, ov_fe_model_path):
    ov_fe_model = core.read_model(onnx_fe_model_path)
    compiled_model = core.compile_model(ov_fe_model, 'CPU')
    ov.save_model(ov_fe_model, ov_fe_model_path)
    print(f"[OpenVINO] feature extraction submodel convert success: {ov_fe_model_path}")

def openvino_model_convert_pose_estimation_submodel(core, ov_pem_input_name, ov_pem_output_name, ov_pem_input, onnx_pem_model_path, ov_pem_model_path, ov_extension_lib_path):
    ov_pem_model = ov.convert_model(onnx_pem_model_path, 
                                    input=ov_pem_input_name,
                                    example_input=ov_pem_input,
                                    extension=ov_extension_lib_path,
                                    )
    compiled_model = core.compile_model(ov_pem_model, 'CPU')
    ov.save_model(ov_pem_model, ov_pem_model_path)
    print(f"[OpenVINO] pose estimation submodel convert success: {ov_pem_model_path}")

def main():
    cfg = init()

    random.seed(cfg.rd_seed)
    torch.manual_seed(cfg.rd_seed)

    # device setting
    device = torch.device(cfg.device)
    print(f"Using device: {device}")

    # model loading
    print("=> creating model ...")
    MODEL = importlib.import_module(cfg.model_name)
    model = MODEL.Net(cfg.model)
    model = model.to(device)
    model.eval()
    checkpoint = os.path.join(os.path.dirname((os.path.abspath(__file__))), 'checkpoints', 'sam-6d-pem-base.pth')
    # load checkpoint
    gorilla.solver.load_checkpoint(model=model, filename=checkpoint, map_location=device)

    # prepare data for model convert
    onnx_pem_input_name, onnx_pem_output_name, onnx_pem_input, \
    onnx_fe_input_name, onnx_fe_output_name, onnx_fe_input, \
    ov_pem_input_name, ov_pem_output_name, ov_pem_input, \
    ov_fe_input_name, ov_fe_output_name, ov_fe_input = prepare_data(cfg, model, device)

    model_save_path = "model_save"
    os.makedirs(model_save_path, exist_ok=True)
    onnx_pem_model_path = os.path.join(model_save_path, 'onnx_pem_model.onnx')
    onnx_fe_model_path = os.path.join(model_save_path, 'onnx_fe_model.onnx')
    ov_pem_model_path = os.path.join(model_save_path, 'ov_pem_model_cpu.xml')
    ov_fe_model_path = os.path.join(model_save_path, 'ov_fe_model_cpu.xml')
    ov_extension_lib_path = './model/ov_pointnet2_op/build/libopenvino_operation_extension.so'

    core = Core()
    core.add_extension(ov_extension_lib_path)

    # onnx model convert
    onnx_model_convert_feature_extraction_submodel(model, onnx_fe_input_name, onnx_fe_output_name, onnx_fe_input, onnx_fe_model_path)
    onnx_model_convert_pose_estimation_submodel(model, onnx_pem_input_name, onnx_pem_output_name, onnx_pem_input, onnx_pem_model_path)

    # # openvino model convert
    openvino_model_convert_feature_extraction_submodel(core, ov_fe_input_name, ov_fe_output_name, ov_fe_input, onnx_fe_model_path, ov_fe_model_path)
    openvino_model_convert_pose_estimation_submodel(core, ov_pem_input_name, ov_pem_output_name, ov_pem_input, onnx_pem_model_path, ov_pem_model_path, ov_extension_lib_path)

if __name__ == "__main__":
    main()